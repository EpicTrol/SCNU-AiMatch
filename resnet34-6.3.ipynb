{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jOP6N55fW_0",
        "colab_type": "code",
        "outputId": "b1b220af-350e-4b73-8f4a-513adc90cad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/aimatch\") \n",
        "!nvidia-smi\n",
        "import torch\n",
        "from torch import nn\n",
        "print(f\"\\n cuda is available:　{torch.cuda.is_available()}\",\n",
        "      f\"\\n device count     :  {torch.cuda.device_count()}\",\n",
        "      f\"\\n device name      :  {torch.cuda.get_device_name(0)}\",)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  4 05:09:33 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            " cuda is available:　True \n",
            " device count     :  1 \n",
            " device name      :  Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ2aJTl-8KYv",
        "colab_type": "code",
        "outputId": "00a5a6c1-1f9b-4232-95f5-6e505c1f49e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python captcha_train_radam.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init net\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value)\n",
            "epoch: 0 step: 10 loss: 0.7630212593988808\n",
            "epoch: 0 step: 20 loss: 0.7265285561157739\n",
            "epoch: 0 step: 30 loss: 0.6802090687813603\n",
            "epoch: 0 step: 40 loss: 0.625851287231116\n",
            "epoch: 0 step: 50 loss: 0.5725389138704023\n",
            "epoch: 0 step: 60 loss: 0.5305556525216885\n",
            "epoch: 0 step: 70 loss: 0.4840668651571218\n",
            "epoch: 0 step: 80 loss: 0.4501543777609649\n",
            "epoch: 0 step: 90 loss: 0.41650301686030494\n",
            "epoch: 0 step: 100 loss: 0.3815495145574555\n",
            "epoch: 0 step: 110 loss: 0.3582948512673733\n",
            "epoch: 0 step: 120 loss: 0.32124655800820084\n",
            "epoch: 0 step: 130 loss: 0.2951253258117866\n",
            "epoch: 0 step: 140 loss: 0.2754313491578646\n",
            "epoch: 0 step: 150 loss: 0.24832769556968454\n",
            "epoch: 0 step: 160 loss: 0.22642204098098703\n",
            "epoch: 0 step: 170 loss: 0.21096092731981259\n",
            "epoch: 0 step: 180 loss: 0.19431877097271102\n",
            "epoch: 0 finished! loss: 0.19431877097271102 learning_rate: [0.0001]\n",
            "epoch: 1 step: 10 loss: 0.17046225866416628\n",
            "epoch: 1 step: 20 loss: 0.15181504272434976\n",
            "epoch: 1 step: 30 loss: 0.13806275377338909\n",
            "epoch: 1 step: 40 loss: 0.13030807938455619\n",
            "epoch: 1 step: 50 loss: 0.1216580041346476\n",
            "epoch: 1 step: 60 loss: 0.11214233051760683\n",
            "epoch: 1 step: 70 loss: 0.11146901921732277\n",
            "epoch: 1 step: 80 loss: 0.10506762475404327\n",
            "epoch: 1 step: 90 loss: 0.10300011126540774\n",
            "epoch: 1 step: 100 loss: 0.0993899136526188\n",
            "epoch: 1 step: 110 loss: 0.09910216951613023\n",
            "epoch: 1 step: 120 loss: 0.09579768034965862\n",
            "epoch: 1 step: 130 loss: 0.09639837638083029\n",
            "epoch: 1 step: 140 loss: 0.0961284228826909\n",
            "epoch: 1 step: 150 loss: 0.09524877272867575\n",
            "epoch: 1 step: 160 loss: 0.09214114768845239\n",
            "epoch: 1 step: 170 loss: 0.09286715495876761\n",
            "epoch: 1 step: 180 loss: 0.0930616996443714\n",
            "epoch: 1 finished! loss: 0.0930616996443714 learning_rate: [0.00017]\n",
            "epoch: 2 step: 10 loss: 0.09030198549206706\n",
            "epoch: 2 step: 20 loss: 0.09243136171956633\n",
            "epoch: 2 step: 30 loss: 0.08961955334958369\n",
            "epoch: 2 step: 40 loss: 0.08974955723139914\n",
            "epoch: 2 step: 50 loss: 0.09026735185371836\n",
            "epoch: 2 step: 60 loss: 0.08996851717039\n",
            "epoch: 2 step: 70 loss: 0.09021370345233802\n",
            "epoch: 2 step: 80 loss: 0.0893740443854816\n",
            "epoch: 2 step: 90 loss: 0.08868455625051916\n",
            "epoch: 2 step: 100 loss: 0.08983995715041919\n",
            "epoch: 2 step: 110 loss: 0.08872256571293025\n",
            "epoch: 2 step: 120 loss: 0.08803714837319834\n",
            "epoch: 2 step: 130 loss: 0.08848946688667336\n",
            "epoch: 2 step: 140 loss: 0.08847670095419782\n",
            "epoch: 2 step: 150 loss: 0.08885744489762148\n",
            "epoch: 2 step: 160 loss: 0.0883468784063159\n",
            "epoch: 2 step: 170 loss: 0.08839696093987608\n",
            "epoch: 2 step: 180 loss: 0.08874681493600953\n",
            "epoch: 2 finished! loss: 0.08874681493600953 learning_rate: [0.00024]\n",
            "epoch: 3 step: 10 loss: 0.08853190859978845\n",
            "epoch: 3 step: 20 loss: 0.0881632547118309\n",
            "epoch: 3 step: 30 loss: 0.08878093833349478\n",
            "epoch: 3 step: 40 loss: 0.08900004416697196\n",
            "epoch: 3 step: 50 loss: 0.08869881927100913\n",
            "epoch: 3 step: 60 loss: 0.08882573396594623\n",
            "epoch: 3 step: 70 loss: 0.08812055903458689\n",
            "epoch: 3 step: 80 loss: 0.08850065729589508\n",
            "epoch: 3 step: 90 loss: 0.08760617796203747\n",
            "epoch: 3 step: 100 loss: 0.08779437840611637\n",
            "epoch: 3 step: 110 loss: 0.08767140378341665\n",
            "epoch: 3 step: 120 loss: 0.08743935812770062\n",
            "epoch: 3 step: 130 loss: 0.08823753194813186\n",
            "epoch: 3 step: 140 loss: 0.08746557790014024\n",
            "epoch: 3 step: 150 loss: 0.0868668991284924\n",
            "epoch: 3 step: 160 loss: 0.0875513010780429\n",
            "epoch: 3 step: 170 loss: 0.086617085830627\n",
            "epoch: 3 step: 180 loss: 0.08678543012289958\n",
            "epoch: 3 finished! loss: 0.08678543012289958 learning_rate: [0.00031]\n",
            "epoch: 4 step: 10 loss: 0.08734556237876705\n",
            "epoch: 4 step: 20 loss: 0.08610224428551887\n",
            "epoch: 4 step: 30 loss: 0.08767317841115664\n",
            "epoch: 4 step: 40 loss: 0.08678299032184665\n",
            "epoch: 4 step: 50 loss: 0.08561167292004475\n",
            "epoch: 4 step: 60 loss: 0.08642760378895864\n",
            "epoch: 4 step: 70 loss: 0.08677328256845267\n",
            "epoch: 4 step: 80 loss: 0.08754097923125531\n",
            "epoch: 4 step: 90 loss: 0.08802357708818635\n",
            "epoch: 4 step: 100 loss: 0.08648374801054443\n",
            "epoch: 4 step: 110 loss: 0.08461035002969429\n",
            "epoch: 4 step: 120 loss: 0.08654952902538021\n",
            "epoch: 4 step: 130 loss: 0.0864479865971325\n",
            "epoch: 4 step: 140 loss: 0.08639914185587005\n",
            "epoch: 4 step: 150 loss: 0.08647457341800156\n",
            "epoch: 4 step: 160 loss: 0.0860047888063143\n",
            "epoch: 4 step: 170 loss: 0.08560969485374344\n",
            "epoch: 4 step: 180 loss: 0.0853445878766261\n",
            "epoch: 4 finished! loss: 0.0853445878766261 learning_rate: [0.00038]\n",
            "epoch: 5 step: 10 loss: 0.08433143410169722\n",
            "epoch: 5 step: 20 loss: 0.08438851726748893\n",
            "epoch: 5 step: 30 loss: 0.08416991907476058\n",
            "epoch: 5 step: 40 loss: 0.08449659931723039\n",
            "epoch: 5 step: 50 loss: 0.08387111809701994\n",
            "epoch: 5 step: 60 loss: 0.08375370189434199\n",
            "epoch: 5 step: 70 loss: 0.08351315077735083\n",
            "epoch: 5 step: 80 loss: 0.08308582062140056\n",
            "epoch: 5 step: 90 loss: 0.08295490717947784\n",
            "epoch: 5 step: 100 loss: 0.08294194687097516\n",
            "epoch: 5 step: 110 loss: 0.08253653228399055\n",
            "epoch: 5 step: 120 loss: 0.08171321563416739\n",
            "epoch: 5 step: 130 loss: 0.0815821230635664\n",
            "epoch: 5 step: 140 loss: 0.080754645968237\n",
            "epoch: 5 step: 150 loss: 0.08035063019884338\n",
            "epoch: 5 step: 160 loss: 0.07960843128586337\n",
            "epoch: 5 step: 170 loss: 0.07969362559020184\n",
            "epoch: 5 step: 180 loss: 0.07793499791817377\n",
            "epoch: 5 finished! loss: 0.07793499791817377 learning_rate: [0.00045000000000000004]\n",
            "epoch: 6 step: 10 loss: 0.07759576538667302\n",
            "epoch: 6 step: 20 loss: 0.07765027916580472\n",
            "epoch: 6 step: 30 loss: 0.07470726269384899\n",
            "epoch: 6 step: 40 loss: 0.07544438329807375\n",
            "epoch: 6 step: 50 loss: 0.07450281265944762\n",
            "epoch: 6 step: 60 loss: 0.07475034615204736\n",
            "epoch: 6 step: 70 loss: 0.07263226131602894\n",
            "epoch: 6 step: 80 loss: 0.07119310502886673\n",
            "epoch: 6 step: 90 loss: 0.07043649491646427\n",
            "epoch: 6 step: 100 loss: 0.06996864648083845\n",
            "epoch: 6 step: 110 loss: 0.06861630109084571\n",
            "epoch: 6 step: 120 loss: 0.0679005799746845\n",
            "epoch: 6 step: 130 loss: 0.06604412874954911\n",
            "epoch: 6 step: 140 loss: 0.06334629225234367\n",
            "epoch: 6 step: 150 loss: 0.06471210729475635\n",
            "epoch: 6 step: 160 loss: 0.062352478464923915\n",
            "epoch: 6 step: 170 loss: 0.06162701092192766\n",
            "epoch: 6 step: 180 loss: 0.060797702605881805\n",
            "epoch: 6 finished! loss: 0.060797702605881805 learning_rate: [0.0005200000000000001]\n",
            "epoch: 7 step: 10 loss: 0.05832384353691417\n",
            "epoch: 7 step: 20 loss: 0.056670055726888095\n",
            "epoch: 7 step: 30 loss: 0.05589313179891174\n",
            "epoch: 7 step: 40 loss: 0.05513521252623906\n",
            "epoch: 7 step: 50 loss: 0.0540264335222832\n",
            "epoch: 7 step: 60 loss: 0.05197283104894509\n",
            "epoch: 7 step: 70 loss: 0.05141038960667585\n",
            "epoch: 7 step: 80 loss: 0.051195651534713574\n",
            "epoch: 7 step: 90 loss: 0.04954089699361697\n",
            "epoch: 7 step: 100 loss: 0.0477202653554447\n",
            "epoch: 7 step: 110 loss: 0.04777201146352852\n",
            "epoch: 7 step: 120 loss: 0.04670316997951476\n",
            "epoch: 7 step: 130 loss: 0.04568251189891999\n",
            "epoch: 7 step: 140 loss: 0.042730863862162446\n",
            "epoch: 7 step: 150 loss: 0.044334818845543425\n",
            "epoch: 7 step: 160 loss: 0.04290016305790348\n",
            "epoch: 7 step: 170 loss: 0.04276057029327299\n",
            "epoch: 7 step: 180 loss: 0.04015826718951392\n",
            "epoch: 7 finished! loss: 0.04015826718951392 learning_rate: [0.00059]\n",
            "epoch: 8 step: 10 loss: 0.04000267747811222\n",
            "epoch: 8 step: 20 loss: 0.03788479032477913\n",
            "epoch: 8 step: 30 loss: 0.03655515897406886\n",
            "epoch: 8 step: 40 loss: 0.03757549642182242\n",
            "epoch: 8 step: 60 loss: 0.034301113514684574\n",
            "epoch: 8 step: 70 loss: 0.034736306813096376\n",
            "epoch: 8 step: 80 loss: 0.03371344355608052\n",
            "epoch: 8 step: 90 loss: 0.031489826676144195\n",
            "epoch: 8 step: 100 loss: 0.03316644350938989\n",
            "epoch: 8 step: 110 loss: 0.031338541300573376\n",
            "epoch: 8 step: 120 loss: 0.029635578968437215\n",
            "epoch: 8 step: 130 loss: 0.031960115979807996\n",
            "epoch: 8 step: 140 loss: 0.030808972178596277\n",
            "epoch: 8 step: 150 loss: 0.02875264834491793\n",
            "epoch: 8 step: 160 loss: 0.027340503353281864\n",
            "epoch: 8 step: 170 loss: 0.027945389576533315\n",
            "epoch: 8 step: 180 loss: 0.02690758377347837\n",
            "epoch: 8 finished! loss: 0.02690758377347837 learning_rate: [0.00066]\n",
            "epoch: 9 step: 10 loss: 0.026146802235135682\n",
            "epoch: 9 step: 20 loss: 0.0249502125852042\n",
            "epoch: 9 step: 30 loss: 0.0254003594705719\n",
            "epoch: 9 step: 40 loss: 0.02350208425908326\n",
            "epoch: 9 step: 50 loss: 0.0254754787987161\n",
            "epoch: 9 step: 60 loss: 0.025537075517688173\n",
            "epoch: 9 step: 70 loss: 0.023876244069767766\n",
            "epoch: 9 step: 80 loss: 0.02384739478800961\n",
            "epoch: 9 step: 90 loss: 0.024210729935746078\n",
            "epoch: 9 step: 100 loss: 0.02105697437282507\n",
            "epoch: 9 step: 110 loss: 0.023124759430360593\n",
            "epoch: 9 step: 120 loss: 0.021407801556358828\n",
            "epoch: 9 step: 130 loss: 0.02147877414443524\n",
            "epoch: 9 step: 140 loss: 0.02212165354294198\n",
            "epoch: 9 step: 150 loss: 0.020646864448451326\n",
            "epoch: 9 step: 160 loss: 0.020332613151933486\n",
            "epoch: 9 step: 170 loss: 0.019034799029249202\n",
            "epoch: 9 step: 180 loss: 0.019485343969776442\n",
            "epoch: 9 finished! loss: 0.019485343969776442 learning_rate: [0.00073]\n",
            "epoch: 10 step: 10 loss: 0.017972488789390124\n",
            "epoch: 10 step: 20 loss: 0.018018524247231514\n",
            "epoch: 10 step: 30 loss: 0.017296236146431006\n",
            "epoch: 10 step: 40 loss: 0.017543647224725845\n",
            "epoch: 10 step: 50 loss: 0.017246085616508647\n",
            "epoch: 10 step: 60 loss: 0.01678066894727363\n",
            "epoch: 10 step: 70 loss: 0.017664284778669492\n",
            "epoch: 10 step: 80 loss: 0.018576978708458525\n",
            "epoch: 10 step: 90 loss: 0.017040906179374052\n",
            "epoch: 10 step: 100 loss: 0.016034057449533647\n",
            "epoch: 10 step: 110 loss: 0.01614018688456354\n",
            "epoch: 10 step: 120 loss: 0.015735410384937837\n",
            "epoch: 10 step: 130 loss: 0.016515167021096903\n",
            "epoch: 10 step: 140 loss: 0.01594219567174101\n",
            "epoch: 10 step: 150 loss: 0.015243931312123194\n",
            "epoch: 10 step: 160 loss: 0.014713097224699717\n",
            "epoch: 10 step: 170 loss: 0.013641612092192484\n",
            "epoch: 10 step: 180 loss: 0.014980476640747567\n",
            "epoch: 10 finished! loss: 0.014980476640747567 learning_rate: [0.0008]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "epoch: 11 step: 10 loss: 0.014061988164697677\n",
            "epoch: 11 step: 20 loss: 0.013571637081951207\n",
            "epoch: 11 step: 30 loss: 0.013145444291339192\n",
            "epoch: 11 step: 40 loss: 0.012750950540587919\n",
            "epoch: 11 step: 50 loss: 0.013302442613567765\n",
            "epoch: 11 step: 60 loss: 0.014469619626567233\n",
            "epoch: 11 step: 70 loss: 0.013663993079043382\n",
            "epoch: 11 step: 80 loss: 0.012623369467891677\n",
            "epoch: 11 step: 90 loss: 0.01265611901473935\n",
            "epoch: 11 step: 100 loss: 0.013035270241305038\n",
            "epoch: 11 step: 110 loss: 0.012090024888939217\n",
            "epoch: 11 step: 120 loss: 0.011735764629478448\n",
            "epoch: 11 step: 130 loss: 0.011642703055852436\n",
            "epoch: 11 step: 140 loss: 0.012566086819502517\n",
            "epoch: 11 step: 150 loss: 0.011515191323781756\n",
            "epoch: 11 step: 160 loss: 0.01217764172050293\n",
            "epoch: 11 step: 170 loss: 0.012983311748041019\n",
            "epoch: 11 step: 180 loss: 0.01229062869116688\n",
            "epoch: 11 finished! loss: 0.01229062869116688 learning_rate: [0.0008]\n",
            "epoch: 12 step: 10 loss: 0.012156609253286934\n",
            "epoch: 12 step: 20 loss: 0.011840586555092656\n",
            "epoch: 12 step: 30 loss: 0.011625333708097863\n",
            "epoch: 12 step: 40 loss: 0.010852234151681402\n",
            "epoch: 12 step: 50 loss: 0.01180177784387248\n",
            "epoch: 12 step: 60 loss: 0.01215658085549982\n",
            "epoch: 12 step: 70 loss: 0.009799813249206715\n",
            "epoch: 12 step: 80 loss: 0.011021294897448458\n",
            "epoch: 12 step: 90 loss: 0.012162179276430249\n",
            "epoch: 12 step: 100 loss: 0.011329353806819377\n",
            "epoch: 12 step: 110 loss: 0.009749432462668371\n",
            "epoch: 12 step: 120 loss: 0.010994764947569347\n",
            "epoch: 12 step: 130 loss: 0.010439378210772364\n",
            "epoch: 12 step: 140 loss: 0.011094293834152877\n",
            "epoch: 12 step: 150 loss: 0.011472213801655157\n",
            "epoch: 12 step: 160 loss: 0.009708880627016329\n",
            "epoch: 12 step: 170 loss: 0.009775102751768575\n",
            "epoch: 12 step: 180 loss: 0.008572230215824247\n",
            "epoch: 12 finished! loss: 0.008572230215824247 learning_rate: [0.0008]\n",
            "epoch: 13 step: 10 loss: 0.00961839657650432\n",
            "epoch: 13 step: 20 loss: 0.00904072502270083\n",
            "epoch: 13 step: 30 loss: 0.009339037879712533\n",
            "epoch: 13 step: 40 loss: 0.008781298852338864\n",
            "epoch: 13 step: 50 loss: 0.009192010913082891\n",
            "epoch: 13 step: 60 loss: 0.008047039190283475\n",
            "epoch: 13 step: 70 loss: 0.008601445916217058\n",
            "epoch: 13 step: 80 loss: 0.00830644205077464\n",
            "epoch: 13 step: 90 loss: 0.0077633282129245555\n",
            "epoch: 13 step: 100 loss: 0.009310578569934105\n",
            "epoch: 13 step: 110 loss: 0.008445764347700512\n",
            "epoch: 13 step: 120 loss: 0.00916608794562253\n",
            "epoch: 13 step: 130 loss: 0.008271215963062574\n",
            "epoch: 13 step: 140 loss: 0.00944011994849107\n",
            "epoch: 13 step: 150 loss: 0.008327089567935177\n",
            "epoch: 13 step: 160 loss: 0.0083557467350098\n",
            "epoch: 13 step: 170 loss: 0.009910322473713068\n",
            "epoch: 13 step: 180 loss: 0.00821581987054967\n",
            "epoch: 13 finished! loss: 0.00821581987054967 learning_rate: [0.0008]\n",
            "epoch: 14 step: 10 loss: 0.009439430087621617\n",
            "epoch: 14 step: 20 loss: 0.008302585605381751\n",
            "epoch: 14 step: 30 loss: 0.00786088181642139\n",
            "epoch: 14 step: 40 loss: 0.006306749671036831\n",
            "epoch: 14 step: 50 loss: 0.007909454059508991\n",
            "epoch: 14 step: 60 loss: 0.008192660409167178\n",
            "epoch: 14 step: 70 loss: 0.008406854782145212\n",
            "epoch: 14 step: 80 loss: 0.008056381159920878\n",
            "epoch: 14 step: 90 loss: 0.007663154099891008\n",
            "epoch: 14 step: 100 loss: 0.007768824387667035\n",
            "epoch: 14 step: 110 loss: 0.0064810190348718036\n",
            "epoch: 14 step: 120 loss: 0.007678885933085745\n",
            "epoch: 14 step: 130 loss: 0.009889292921785498\n",
            "epoch: 14 step: 140 loss: 0.008122647096597334\n",
            "epoch: 14 step: 150 loss: 0.007990958614643657\n",
            "epoch: 14 step: 160 loss: 0.00862553704727624\n",
            "epoch: 14 step: 170 loss: 0.008393731725170444\n",
            "epoch: 14 step: 180 loss: 0.007226352215941613\n",
            "epoch: 14 finished! loss: 0.007226352215941613 learning_rate: [0.0008]\n",
            "epoch: 15 step: 10 loss: 0.00763715559875714\n",
            "epoch: 15 step: 20 loss: 0.007187427555031688\n",
            "epoch: 15 step: 30 loss: 0.006379016137067978\n",
            "epoch: 15 step: 40 loss: 0.006773616751546759\n",
            "epoch: 15 step: 50 loss: 0.0065448336622652694\n",
            "epoch: 15 step: 60 loss: 0.007101817709610242\n",
            "epoch: 15 step: 70 loss: 0.0064986701303672454\n",
            "epoch: 15 step: 80 loss: 0.005363944127920376\n",
            "epoch: 15 step: 90 loss: 0.006797093896310455\n",
            "epoch: 15 step: 100 loss: 0.008084964118487657\n",
            "epoch: 15 step: 110 loss: 0.008450351607488402\n",
            "epoch: 15 step: 120 loss: 0.010019532221223095\n",
            "epoch: 15 step: 130 loss: 0.006806449741069031\n",
            "epoch: 15 step: 140 loss: 0.006565261816820631\n",
            "epoch: 15 step: 150 loss: 0.006035268999189524\n",
            "epoch: 15 step: 160 loss: 0.006309045559631766\n",
            "epoch: 15 step: 170 loss: 0.006817942360220869\n",
            "epoch: 15 step: 180 loss: 0.008201096682164049\n",
            "epoch: 15 finished! loss: 0.008201096682164049 learning_rate: [0.0008]\n",
            "epoch: 16 step: 10 loss: 0.006611396262642237\n",
            "epoch: 16 step: 20 loss: 0.007656404234167412\n",
            "epoch: 16 step: 30 loss: 0.006811309863719169\n",
            "epoch: 16 step: 40 loss: 0.005131824962430111\n",
            "epoch: 16 step: 50 loss: 0.006774734504377708\n",
            "epoch: 16 step: 60 loss: 0.004722702926748747\n",
            "epoch: 16 step: 70 loss: 0.00518964586133184\n",
            "epoch: 16 step: 80 loss: 0.005600683576004955\n",
            "epoch: 16 step: 90 loss: 0.008032299036115912\n",
            "epoch: 16 step: 100 loss: 0.0065428184616170915\n",
            "epoch: 16 step: 110 loss: 0.007105656011484911\n",
            "epoch: 16 step: 120 loss: 0.006080544742856408\n",
            "epoch: 16 step: 130 loss: 0.00630601794516366\n",
            "epoch: 16 step: 140 loss: 0.006722347861777121\n",
            "epoch: 16 step: 150 loss: 0.009714026938266288\n",
            "epoch: 16 step: 160 loss: 0.007106901767636205\n",
            "epoch: 16 step: 170 loss: 0.005538242754147401\n",
            "epoch: 16 step: 180 loss: 0.0057279061343907985\n",
            "epoch: 16 finished! loss: 0.0057279061343907985 learning_rate: [0.0008]\n",
            "epoch: 17 step: 10 loss: 0.005940698513500931\n",
            "epoch: 17 step: 20 loss: 0.0055057798846573445\n",
            "epoch: 17 step: 30 loss: 0.005480401867146692\n",
            "epoch: 17 step: 40 loss: 0.006694882474313658\n",
            "epoch: 17 step: 50 loss: 0.00838818729539462\n",
            "epoch: 17 step: 60 loss: 0.006348003318748654\n",
            "epoch: 17 step: 70 loss: 0.005156697667070439\n",
            "epoch: 17 step: 80 loss: 0.006103570639098146\n",
            "epoch: 17 step: 90 loss: 0.00577335660164886\n",
            "epoch: 17 step: 100 loss: 0.004472535115624539\n",
            "epoch: 17 step: 110 loss: 0.0063581709656754005\n",
            "epoch: 17 step: 120 loss: 0.0061996619495846665\n",
            "epoch: 17 step: 130 loss: 0.004915478038358703\n",
            "epoch: 17 step: 140 loss: 0.006538297086151108\n",
            "epoch: 17 step: 150 loss: 0.005080515221233983\n",
            "epoch: 17 step: 160 loss: 0.005259859316302179\n",
            "epoch: 17 step: 170 loss: 0.00426156314251842\n",
            "epoch: 17 step: 180 loss: 0.006231991323838234\n",
            "epoch: 17 finished! loss: 0.006231991323838234 learning_rate: [0.0008]\n",
            "epoch: 18 step: 10 loss: 0.004759730014097246\n",
            "epoch: 18 step: 20 loss: 0.004678468989141435\n",
            "epoch: 18 step: 30 loss: 0.005270060272535426\n",
            "epoch: 18 step: 40 loss: 0.004828664464340952\n",
            "epoch: 18 step: 50 loss: 0.005162315718935649\n",
            "epoch: 18 step: 60 loss: 0.006691101591240089\n",
            "epoch: 18 step: 70 loss: 0.0062239927367367795\n",
            "epoch: 18 step: 80 loss: 0.005993223422639212\n",
            "epoch: 18 step: 90 loss: 0.005690836317241216\n",
            "epoch: 18 step: 100 loss: 0.006453954303468953\n",
            "epoch: 18 step: 110 loss: 0.005611533895885931\n",
            "epoch: 18 step: 120 loss: 0.004181179234857415\n",
            "epoch: 18 step: 130 loss: 0.0047793971498112445\n",
            "epoch: 18 step: 140 loss: 0.006384762539790013\n",
            "epoch: 18 step: 150 loss: 0.005389803898030826\n",
            "epoch: 18 step: 160 loss: 0.005475451303300842\n",
            "epoch: 18 step: 170 loss: 0.0046635975160807825\n",
            "epoch: 18 step: 180 loss: 0.004961631565816632\n",
            "epoch: 18 finished! loss: 0.004961631565816632 learning_rate: [0.0008]\n",
            "epoch: 19 step: 10 loss: 0.00631758200100245\n",
            "epoch: 19 step: 20 loss: 0.0048023527257412555\n",
            "epoch: 19 step: 30 loss: 0.004246089962851894\n",
            "epoch: 19 step: 40 loss: 0.004444699289886717\n",
            "epoch: 19 step: 50 loss: 0.004603150735394598\n",
            "epoch: 19 step: 60 loss: 0.005226780942515772\n",
            "epoch: 19 step: 70 loss: 0.0062301706799449234\n",
            "epoch: 19 step: 80 loss: 0.0057375809558172265\n",
            "epoch: 19 step: 90 loss: 0.0043756977074153715\n",
            "epoch: 19 step: 100 loss: 0.0053886546866929955\n",
            "epoch: 19 step: 110 loss: 0.005913774097490837\n",
            "epoch: 19 step: 120 loss: 0.0049542544154430315\n",
            "epoch: 19 step: 130 loss: 0.005203844293908604\n",
            "epoch: 19 step: 140 loss: 0.0048355041117731454\n",
            "epoch: 19 step: 150 loss: 0.005886625830134731\n",
            "epoch: 19 step: 160 loss: 0.004581790381070256\n",
            "epoch: 19 step: 170 loss: 0.004842954461237965\n",
            "epoch: 19 step: 180 loss: 0.003946902240794235\n",
            "epoch: 19 finished! loss: 0.003946902240794235 learning_rate: [0.0008]\n",
            "epoch: 20 step: 10 loss: 0.0047497529939069085\n",
            "epoch: 20 step: 20 loss: 0.004396102963616997\n",
            "epoch: 20 step: 30 loss: 0.00316191992846566\n",
            "epoch: 20 step: 40 loss: 0.003614210548278052\n",
            "epoch: 20 step: 50 loss: 0.004277050742257362\n",
            "epoch: 20 step: 60 loss: 0.0042361075417410065\n",
            "epoch: 20 step: 70 loss: 0.004678740523491895\n",
            "epoch: 20 step: 80 loss: 0.004780507142129773\n",
            "epoch: 20 step: 90 loss: 0.0046812342160112074\n",
            "epoch: 20 step: 100 loss: 0.0038607446276374877\n",
            "epoch: 20 step: 110 loss: 0.0038662440439245467\n",
            "epoch: 20 step: 120 loss: 0.004384283082014253\n",
            "epoch: 20 step: 130 loss: 0.0049903939984566326\n",
            "epoch: 20 step: 140 loss: 0.004205222818714527\n",
            "epoch: 20 step: 150 loss: 0.003984783725324375\n",
            "epoch: 20 step: 160 loss: 0.00366952074063905\n",
            "epoch: 20 step: 170 loss: 0.004750642837614878\n",
            "epoch: 20 step: 180 loss: 0.0049525219161180795\n",
            "epoch: 20 finished! loss: 0.0049525219161180795 learning_rate: [0.0008]\n",
            "epoch: 21 step: 10 loss: 0.004534169252968499\n",
            "epoch: 21 step: 20 loss: 0.003720776434860314\n",
            "epoch: 21 step: 30 loss: 0.00446116573464909\n",
            "epoch: 21 step: 40 loss: 0.0035446627140523694\n",
            "epoch: 21 step: 50 loss: 0.003721021994912223\n",
            "epoch: 21 step: 60 loss: 0.003815125097068589\n",
            "epoch: 21 step: 70 loss: 0.0037703832329412743\n",
            "epoch: 21 step: 80 loss: 0.004809440719883387\n",
            "epoch: 21 step: 90 loss: 0.004419316519777113\n",
            "epoch: 21 step: 100 loss: 0.004699283798851537\n",
            "epoch: 21 step: 110 loss: 0.004887482646750088\n",
            "epoch: 21 step: 120 loss: 0.003771970379105072\n",
            "epoch: 21 step: 130 loss: 0.005379781311789795\n",
            "epoch: 21 step: 140 loss: 0.003655832489475342\n",
            "epoch: 21 step: 150 loss: 0.0036380158669358275\n",
            "epoch: 21 step: 160 loss: 0.005720901450259278\n",
            "epoch: 21 step: 170 loss: 0.003667132800439477\n",
            "epoch: 21 step: 180 loss: 0.0036965430025118254\n",
            "epoch: 21 finished! loss: 0.0036965430025118254 learning_rate: [0.0008]\n",
            "epoch: 22 step: 10 loss: 0.0038818631454887688\n",
            "epoch: 22 step: 20 loss: 0.0036105494736142153\n",
            "epoch: 22 step: 30 loss: 0.002554781903369693\n",
            "epoch: 22 step: 40 loss: 0.0034924794547832936\n",
            "epoch: 22 step: 50 loss: 0.004059660159073185\n",
            "epoch: 22 step: 60 loss: 0.0032292213167648114\n",
            "epoch: 22 step: 70 loss: 0.004839959713224693\n",
            "epoch: 22 step: 80 loss: 0.0047959191407151115\n",
            "epoch: 22 step: 90 loss: 0.004939459435092441\n",
            "epoch: 22 step: 100 loss: 0.004226481633367287\n",
            "epoch: 22 step: 110 loss: 0.0038006876217626935\n",
            "epoch: 22 step: 120 loss: 0.004800641740342302\n",
            "epoch: 22 step: 130 loss: 0.003578986318075032\n",
            "epoch: 22 step: 140 loss: 0.0045197874615564276\n",
            "epoch: 22 step: 150 loss: 0.005622904665611601\n",
            "epoch: 22 step: 160 loss: 0.004674407024113619\n",
            "epoch: 22 step: 170 loss: 0.0045482935981197145\n",
            "epoch: 22 step: 180 loss: 0.00483526004919912\n",
            "epoch: 22 finished! loss: 0.00483526004919912 learning_rate: [0.0008]\n",
            "epoch: 23 step: 10 loss: 0.003319684105464373\n",
            "epoch: 23 step: 20 loss: 0.004323153449818067\n",
            "epoch: 23 step: 30 loss: 0.003524204523627265\n",
            "epoch: 23 step: 40 loss: 0.00340352015670608\n",
            "epoch: 23 step: 50 loss: 0.0032974517188835224\n",
            "epoch: 23 step: 60 loss: 0.004209179240760033\n",
            "epoch: 23 step: 70 loss: 0.004673966233464591\n",
            "epoch: 23 step: 80 loss: 0.0035036826550753947\n",
            "epoch: 23 step: 90 loss: 0.0035592210880816817\n",
            "epoch: 23 step: 100 loss: 0.003897971265327066\n",
            "epoch: 23 step: 110 loss: 0.003977185013776782\n",
            "epoch: 23 step: 120 loss: 0.004209386931161192\n",
            "epoch: 23 step: 130 loss: 0.0048516960089446\n",
            "epoch: 23 step: 140 loss: 0.003953798925076307\n",
            "epoch: 23 step: 150 loss: 0.002919252907180074\n",
            "epoch: 23 step: 160 loss: 0.0030335422827490505\n",
            "epoch: 23 step: 170 loss: 0.0031479106943778945\n",
            "epoch: 23 step: 180 loss: 0.005602056340402682\n",
            "epoch: 23 finished! loss: 0.005602056340402682 learning_rate: [0.0008]\n",
            "epoch: 24 step: 10 loss: 0.003902538016937776\n",
            "epoch: 24 step: 20 loss: 0.00556371809747699\n",
            "epoch: 24 step: 30 loss: 0.0039935889188083655\n",
            "epoch: 24 step: 40 loss: 0.0032232091768557114\n",
            "epoch: 24 step: 50 loss: 0.0036702953843815956\n",
            "epoch: 24 step: 60 loss: 0.003453020682814275\n",
            "epoch: 24 step: 70 loss: 0.004167016986206871\n",
            "epoch: 24 step: 80 loss: 0.004166908986393985\n",
            "epoch: 24 step: 90 loss: 0.004843214273260224\n",
            "epoch: 24 step: 100 loss: 0.0038029187033366374\n",
            "epoch: 24 step: 110 loss: 0.003897231211008234\n",
            "epoch: 24 step: 120 loss: 0.002522647013120707\n",
            "epoch: 24 step: 130 loss: 0.004756772634309892\n",
            "epoch: 24 step: 140 loss: 0.004010373762709077\n",
            "epoch: 24 step: 150 loss: 0.003781541539320588\n",
            "epoch: 24 step: 160 loss: 0.0034007347415543907\n",
            "epoch: 24 step: 170 loss: 0.0033629998141326447\n",
            "epoch: 24 step: 180 loss: 0.0030334038089816886\n",
            "epoch: 24 finished! loss: 0.0030334038089816886 learning_rate: [0.0008]\n",
            "epoch: 25 step: 10 loss: 0.0035191487150548393\n",
            "epoch: 25 step: 20 loss: 0.0025285839845785164\n",
            "epoch: 25 step: 30 loss: 0.002338678701090735\n",
            "epoch: 25 step: 40 loss: 0.0027634318322951085\n",
            "epoch: 25 step: 50 loss: 0.0016260512474259323\n",
            "epoch: 25 step: 60 loss: 0.0028365323121209846\n",
            "epoch: 25 step: 70 loss: 0.0032988545049172343\n",
            "epoch: 25 step: 80 loss: 0.0023274361790392387\n",
            "epoch: 25 step: 90 loss: 0.004117253195818607\n",
            "epoch: 25 step: 100 loss: 0.003191949185797815\n",
            "epoch: 25 step: 110 loss: 0.0028762494504118905\n",
            "epoch: 25 step: 120 loss: 0.004080602565173249\n",
            "epoch: 25 step: 130 loss: 0.0025237923688751288\n",
            "epoch: 25 step: 140 loss: 0.005061844089980957\n",
            "epoch: 25 step: 150 loss: 0.004930862256385799\n",
            "epoch: 25 step: 160 loss: 0.0052245556951579526\n",
            "epoch: 25 step: 170 loss: 0.0036582391873466015\n",
            "epoch: 25 step: 180 loss: 0.0045986110947085525\n",
            "epoch: 25 finished! loss: 0.0045986110947085525 learning_rate: [0.0008]\n",
            "epoch: 26 step: 10 loss: 0.004580159254297798\n",
            "epoch: 26 step: 20 loss: 0.004246700540370799\n",
            "epoch: 26 step: 30 loss: 0.0032054210006684323\n",
            "epoch: 26 step: 40 loss: 0.0028410694501739484\n",
            "epoch: 26 step: 50 loss: 0.003292627032569986\n",
            "epoch: 26 step: 60 loss: 0.002625790801823855\n",
            "epoch: 26 step: 70 loss: 0.003447729591244712\n",
            "epoch: 26 step: 80 loss: 0.0027235661621485474\n",
            "epoch: 26 step: 90 loss: 0.0021595924511608015\n",
            "epoch: 26 step: 100 loss: 0.0029777477821297644\n",
            "epoch: 26 step: 110 loss: 0.002866847981472543\n",
            "epoch: 26 step: 120 loss: 0.002868768612566762\n",
            "epoch: 26 step: 130 loss: 0.0039472319428585445\n",
            "epoch: 26 step: 140 loss: 0.0034920920027655435\n",
            "epoch: 26 step: 150 loss: 0.003754260294045422\n",
            "epoch: 26 step: 160 loss: 0.0036896105650303405\n",
            "epoch: 26 step: 170 loss: 0.0031123440988854918\n",
            "epoch: 26 step: 180 loss: 0.0030480368356048592\n",
            "epoch: 26 finished! loss: 0.0030480368356048592 learning_rate: [0.0008]\n",
            "epoch: 27 step: 10 loss: 0.003054432432542416\n",
            "epoch: 27 step: 20 loss: 0.0023669775225583444\n",
            "epoch: 27 step: 30 loss: 0.002811504799082817\n",
            "epoch: 27 step: 40 loss: 0.0023640177763885376\n",
            "epoch: 27 step: 50 loss: 0.0026334713033773043\n",
            "epoch: 27 step: 60 loss: 0.003334025847228334\n",
            "epoch: 27 step: 70 loss: 0.0031261624310597834\n",
            "epoch: 27 step: 80 loss: 0.0034960877016289686\n",
            "epoch: 27 step: 90 loss: 0.002139996054051906\n",
            "epoch: 27 step: 100 loss: 0.0033994148347183096\n",
            "epoch: 27 step: 110 loss: 0.0031566591999881207\n",
            "epoch: 27 step: 120 loss: 0.003629770632089943\n",
            "epoch: 27 step: 130 loss: 0.0036282700398790725\n",
            "epoch: 27 step: 140 loss: 0.0037371410679809835\n",
            "epoch: 27 step: 150 loss: 0.0035898033370987343\n",
            "epoch: 27 step: 160 loss: 0.004340425729768943\n",
            "epoch: 27 step: 170 loss: 0.002974615196190918\n",
            "epoch: 27 step: 180 loss: 0.003174047209697048\n",
            "epoch: 27 finished! loss: 0.003174047209697048 learning_rate: [0.0008]\n",
            "epoch: 28 step: 10 loss: 0.002266698315941403\n",
            "epoch: 28 step: 20 loss: 0.0030524459758307023\n",
            "epoch: 28 step: 30 loss: 0.0021747865425680575\n",
            "epoch: 28 step: 40 loss: 0.003272270911217413\n",
            "epoch: 28 step: 50 loss: 0.004627840812421676\n",
            "epoch: 28 step: 60 loss: 0.004388020539278209\n",
            "epoch: 28 step: 70 loss: 0.0040245576405775725\n",
            "epoch: 28 step: 80 loss: 0.0045930654221725984\n",
            "epoch: 28 step: 90 loss: 0.0030697845447200983\n",
            "epoch: 28 step: 100 loss: 0.003912802775123818\n",
            "epoch: 28 step: 110 loss: 0.002374883737362035\n",
            "epoch: 28 step: 120 loss: 0.00448716554052866\n",
            "epoch: 28 step: 130 loss: 0.003200878464470465\n",
            "epoch: 28 step: 140 loss: 0.0035479974244947477\n",
            "epoch: 28 step: 150 loss: 0.002359933057877296\n",
            "epoch: 28 step: 160 loss: 0.0024428721395678035\n",
            "epoch: 28 step: 170 loss: 0.003884319160129444\n",
            "epoch: 28 step: 180 loss: 0.002440378494267343\n",
            "epoch: 28 finished! loss: 0.002440378494267343 learning_rate: [0.0008]\n",
            "epoch: 29 step: 10 loss: 0.002645087628388056\n",
            "epoch: 29 step: 20 loss: 0.0018890387842787407\n",
            "epoch: 29 step: 30 loss: 0.002424738182835263\n",
            "epoch: 29 step: 40 loss: 0.004019873791594303\n",
            "epoch: 29 step: 50 loss: 0.003144433401860441\n",
            "epoch: 29 step: 60 loss: 0.002547391823668588\n",
            "epoch: 29 step: 70 loss: 0.0023067526825553937\n",
            "epoch: 29 step: 80 loss: 0.0026676412330422354\n",
            "epoch: 29 step: 90 loss: 0.003210077998456177\n",
            "epoch: 29 step: 100 loss: 0.00284035374451038\n",
            "epoch: 29 step: 110 loss: 0.0022202990768649686\n",
            "epoch: 29 step: 120 loss: 0.00234251159310547\n",
            "epoch: 29 step: 130 loss: 0.0028988921343571796\n",
            "epoch: 29 step: 140 loss: 0.0031102128558966632\n",
            "epoch: 29 step: 150 loss: 0.003816273582602434\n",
            "epoch: 29 step: 160 loss: 0.002718215562014155\n",
            "epoch: 29 step: 170 loss: 0.003356626711282533\n",
            "epoch: 29 step: 180 loss: 0.003831381479420075\n",
            "epoch: 29 finished! loss: 0.003831381479420075 learning_rate: [0.0008]\n",
            "epoch: 30 step: 10 loss: 0.0017561804037549346\n",
            "epoch: 30 step: 20 loss: 0.0026891942403344956\n",
            "epoch: 30 step: 30 loss: 0.004220228469849997\n",
            "epoch: 30 step: 40 loss: 0.00225697682313343\n",
            "epoch: 30 step: 50 loss: 0.002867480227306266\n",
            "epoch: 30 step: 60 loss: 0.0026223771728477612\n",
            "epoch: 30 step: 70 loss: 0.002828500904620601\n",
            "epoch: 30 step: 80 loss: 0.0023349465760450824\n",
            "epoch: 30 step: 90 loss: 0.002864816746303765\n",
            "epoch: 30 step: 100 loss: 0.0031969384028451707\n",
            "epoch: 30 step: 110 loss: 0.002609181108458272\n",
            "epoch: 30 step: 120 loss: 0.002201543130208315\n",
            "epoch: 30 step: 130 loss: 0.002475262255216557\n",
            "epoch: 30 step: 140 loss: 0.0023780351452082327\n",
            "epoch: 30 step: 150 loss: 0.0029354177163864945\n",
            "epoch: 30 step: 160 loss: 0.00317476574730135\n",
            "epoch: 30 step: 170 loss: 0.0033266275629633612\n",
            "epoch: 30 step: 180 loss: 0.0019920658633822462\n",
            "epoch: 30 finished! loss: 0.0019920658633822462 learning_rate: [0.0008]\n",
            "epoch: 31 step: 10 loss: 0.0019642272515230748\n",
            "epoch: 31 step: 20 loss: 0.001378877696459404\n",
            "epoch: 31 step: 30 loss: 0.001085763470453055\n",
            "epoch: 31 step: 40 loss: 0.0017190672831293074\n",
            "epoch: 31 step: 50 loss: 0.001957467313228217\n",
            "epoch: 31 step: 60 loss: 0.0014597584328119387\n",
            "epoch: 31 step: 70 loss: 0.0009423173736843144\n",
            "epoch: 31 step: 80 loss: 0.0011742366231944786\n",
            "epoch: 31 step: 90 loss: 0.0011668838544101824\n",
            "epoch: 31 step: 100 loss: 0.0015989936776437958\n",
            "epoch: 31 step: 110 loss: 0.001180077420868453\n",
            "epoch: 31 step: 120 loss: 0.001382559526071203\n",
            "epoch: 31 step: 130 loss: 0.0013473692114535066\n",
            "epoch: 31 step: 140 loss: 0.0009998995287669097\n",
            "epoch: 31 step: 150 loss: 0.0010169839521949817\n",
            "epoch: 31 step: 160 loss: 0.0012453909174749474\n",
            "epoch: 31 step: 170 loss: 0.001513796406933388\n",
            "epoch: 31 step: 180 loss: 0.0012624131838122324\n",
            "epoch: 31 finished! loss: 0.0012624131838122324 learning_rate: [0.0002]\n",
            "epoch: 32 step: 10 loss: 0.001212244230241859\n",
            "epoch: 32 step: 20 loss: 0.0010035739949721043\n",
            "epoch: 32 step: 30 loss: 0.0007493068661638757\n",
            "epoch: 32 step: 40 loss: 0.000807211819814793\n",
            "epoch: 32 step: 50 loss: 0.0009296230183412813\n",
            "epoch: 32 step: 60 loss: 0.0010703068593296019\n",
            "epoch: 32 step: 70 loss: 0.0011191789018640654\n",
            "epoch: 32 step: 80 loss: 0.0016179915205723737\n",
            "epoch: 32 step: 90 loss: 0.0014855700685266647\n",
            "epoch: 32 step: 100 loss: 0.0009938787469605155\n",
            "epoch: 32 step: 110 loss: 0.0009578670068788444\n",
            "epoch: 32 step: 120 loss: 0.002040351053590634\n",
            "epoch: 32 step: 130 loss: 0.0010903642305782183\n",
            "epoch: 32 step: 140 loss: 0.0012962153292351375\n",
            "epoch: 32 step: 150 loss: 0.001243961622875497\n",
            "epoch: 32 step: 160 loss: 0.0010432168170285015\n",
            "epoch: 32 step: 170 loss: 0.0007972471920014478\n",
            "epoch: 32 step: 180 loss: 0.001248035079820134\n",
            "epoch: 32 finished! loss: 0.001248035079820134 learning_rate: [0.0004]\n",
            "epoch: 33 step: 10 loss: 0.0015157737927600536\n",
            "epoch: 33 step: 20 loss: 0.0009631958605503256\n",
            "epoch: 33 step: 30 loss: 0.0010364615683753548\n",
            "epoch: 33 step: 40 loss: 0.0007501840266119813\n",
            "epoch: 33 step: 50 loss: 0.0013083386148783592\n",
            "epoch: 33 step: 60 loss: 0.0011774772167212468\n",
            "epoch: 33 step: 70 loss: 0.000880550922061961\n",
            "epoch: 33 step: 80 loss: 0.0009165205105623686\n",
            "epoch: 33 step: 90 loss: 0.00110725139370491\n",
            "epoch: 33 step: 100 loss: 0.0015211612907784164\n",
            "epoch: 33 step: 110 loss: 0.0008714657158358178\n",
            "epoch: 33 step: 120 loss: 0.0014368333946555654\n",
            "epoch: 33 step: 130 loss: 0.0020857837683520685\n",
            "epoch: 33 step: 140 loss: 0.0009224530257252398\n",
            "epoch: 33 step: 150 loss: 0.0009148934927369695\n",
            "epoch: 33 step: 160 loss: 0.0010006254100561576\n",
            "epoch: 33 step: 170 loss: 0.0012680024701620834\n",
            "epoch: 33 step: 180 loss: 0.0008934625597412738\n",
            "epoch: 33 finished! loss: 0.0008934625597412738 learning_rate: [0.0004]\n",
            "epoch: 34 step: 10 loss: 0.000978516057025013\n",
            "epoch: 34 step: 20 loss: 0.0011306630639216402\n",
            "epoch: 34 step: 30 loss: 0.0009752043331948399\n",
            "epoch: 34 step: 40 loss: 0.0011939239786376572\n",
            "epoch: 34 step: 50 loss: 0.0012119448401773539\n",
            "epoch: 34 step: 60 loss: 0.0011243546334145287\n",
            "epoch: 34 step: 70 loss: 0.0013484888533487592\n",
            "epoch: 34 step: 80 loss: 0.0013437527411242294\n",
            "epoch: 34 step: 90 loss: 0.0008607319903286468\n",
            "epoch: 34 step: 100 loss: 0.0016262114333870468\n",
            "epoch: 34 step: 110 loss: 0.001280295568587521\n",
            "epoch: 34 step: 120 loss: 0.0012969079150331579\n",
            "epoch: 34 step: 130 loss: 0.0013107465383656492\n",
            "epoch: 34 step: 140 loss: 0.000748653245832327\n",
            "epoch: 34 step: 150 loss: 0.0013452562260074903\n",
            "epoch: 34 step: 160 loss: 0.0008704855019126916\n",
            "epoch: 34 step: 170 loss: 0.0010195063677289674\n",
            "epoch: 34 step: 180 loss: 0.0009558874331065482\n",
            "epoch: 34 finished! loss: 0.0009558874331065482 learning_rate: [0.0004]\n",
            "epoch: 35 step: 10 loss: 0.0009184125505264828\n",
            "epoch: 35 step: 20 loss: 0.0011219976101433144\n",
            "epoch: 35 step: 30 loss: 0.00102487709634902\n",
            "epoch: 35 step: 40 loss: 0.00107221676291776\n",
            "epoch: 35 step: 50 loss: 0.0007175000685409564\n",
            "epoch: 35 step: 60 loss: 0.0007969677052739854\n",
            "epoch: 35 step: 70 loss: 0.00088178317430084\n",
            "epoch: 35 step: 80 loss: 0.001320331617963845\n",
            "epoch: 35 step: 90 loss: 0.0010083019534811969\n",
            "epoch: 35 step: 100 loss: 0.0009707024754426952\n",
            "epoch: 35 step: 110 loss: 0.0011220391278481926\n",
            "epoch: 35 step: 120 loss: 0.0016191994578217894\n",
            "epoch: 35 step: 130 loss: 0.002069413747978281\n",
            "epoch: 35 step: 140 loss: 0.001148842968416605\n",
            "epoch: 35 step: 150 loss: 0.000958117203177997\n",
            "epoch: 35 step: 160 loss: 0.0008248337915257823\n",
            "epoch: 35 step: 170 loss: 0.0010797981253022465\n",
            "epoch: 35 step: 180 loss: 0.0012805177817468358\n",
            "epoch: 35 finished! loss: 0.0012805177817468358 learning_rate: [0.0004]\n",
            "epoch: 36 step: 10 loss: 0.0015485384505604615\n",
            "epoch: 36 step: 20 loss: 0.0012958876228512152\n",
            "epoch: 36 step: 30 loss: 0.0009110694935726552\n",
            "epoch: 36 step: 40 loss: 0.0009054553663577695\n",
            "epoch: 36 step: 50 loss: 0.0021417116639384577\n",
            "epoch: 36 step: 60 loss: 0.001128995217036592\n",
            "epoch: 36 step: 70 loss: 0.0008694080341626622\n",
            "epoch: 36 step: 80 loss: 0.0014728456699317693\n",
            "epoch: 36 step: 90 loss: 0.001119380717184973\n",
            "epoch: 36 step: 100 loss: 0.00076466617968868\n",
            "epoch: 36 step: 110 loss: 0.0012414574136607257\n",
            "epoch: 36 step: 120 loss: 0.0008907360596694781\n",
            "epoch: 36 step: 130 loss: 0.001341506067459294\n",
            "epoch: 36 step: 140 loss: 0.0009956095276063461\n",
            "epoch: 36 step: 150 loss: 0.0006397654568618642\n",
            "epoch: 36 step: 160 loss: 0.0008368246134188335\n",
            "epoch: 36 step: 170 loss: 0.0009740324443177793\n",
            "epoch: 36 step: 180 loss: 0.0011413695896737401\n",
            "epoch: 36 finished! loss: 0.0011413695896737401 learning_rate: [0.0004]\n",
            "epoch: 37 step: 10 loss: 0.0013677426676666263\n",
            "epoch: 37 step: 20 loss: 0.0016422008209318038\n",
            "epoch: 37 step: 30 loss: 0.0012683911864782927\n",
            "epoch: 37 step: 40 loss: 0.0010526189093288268\n",
            "epoch: 37 step: 50 loss: 0.001120897751674273\n",
            "epoch: 37 step: 60 loss: 0.0010551217898231197\n",
            "epoch: 37 step: 70 loss: 0.0008665056238902918\n",
            "epoch: 37 step: 80 loss: 0.000804411471603341\n",
            "epoch: 37 step: 90 loss: 0.0011304717710885722\n",
            "epoch: 37 step: 100 loss: 0.0013366856719019002\n",
            "epoch: 37 step: 110 loss: 0.0008849089424325591\n",
            "epoch: 37 step: 120 loss: 0.0009724053423576014\n",
            "epoch: 37 step: 130 loss: 0.0016880968787297947\n",
            "epoch: 37 step: 140 loss: 0.0007923945361247985\n",
            "epoch: 37 step: 150 loss: 0.0006226807206686432\n",
            "epoch: 37 step: 160 loss: 0.0007605317470196473\n",
            "epoch: 37 step: 170 loss: 0.00118858550938551\n",
            "epoch: 37 step: 180 loss: 0.0007455640818509838\n",
            "epoch: 37 finished! loss: 0.0007455640818509838 learning_rate: [0.0004]\n",
            "epoch: 38 step: 10 loss: 0.001980182899729399\n",
            "epoch: 38 step: 20 loss: 0.001348759374370947\n",
            "epoch: 38 step: 30 loss: 0.0014366370670446781\n",
            "epoch: 38 step: 40 loss: 0.001692519252141063\n",
            "epoch: 38 step: 50 loss: 0.0019343779000399895\n",
            "epoch: 38 step: 60 loss: 0.0011682614435062264\n",
            "epoch: 38 step: 70 loss: 0.001408863413133918\n",
            "epoch: 38 step: 80 loss: 0.0012882247062179332\n",
            "epoch: 38 step: 90 loss: 0.0006250477015016311\n",
            "epoch: 38 step: 100 loss: 0.0008883777655241904\n",
            "epoch: 38 step: 110 loss: 0.0004531516301589525\n",
            "epoch: 38 step: 120 loss: 0.0012496732891612325\n",
            "epoch: 38 step: 130 loss: 0.0032511004093207386\n",
            "epoch: 38 step: 140 loss: 0.0012997580993003776\n",
            "epoch: 38 step: 150 loss: 0.0013400145039744538\n",
            "epoch: 38 step: 160 loss: 0.001416421684371769\n",
            "epoch: 38 step: 170 loss: 0.0008530834586956363\n",
            "epoch: 38 step: 180 loss: 0.0015848779420110208\n",
            "epoch: 38 finished! loss: 0.0015848779420110208 learning_rate: [0.0004]\n",
            "epoch: 39 step: 10 loss: 0.0011311205536465529\n",
            "epoch: 39 step: 20 loss: 0.00203575754211559\n",
            "epoch: 39 step: 30 loss: 0.001504770184161986\n",
            "epoch: 39 step: 40 loss: 0.001064342057297817\n",
            "epoch: 39 step: 50 loss: 0.001404013084098508\n",
            "epoch: 39 step: 60 loss: 0.001969496937658888\n",
            "epoch: 39 step: 70 loss: 0.0012882325530404533\n",
            "epoch: 39 step: 80 loss: 0.0009180215762123443\n",
            "epoch: 39 step: 90 loss: 0.001388709556099255\n",
            "epoch: 39 step: 100 loss: 0.00198687999302519\n",
            "epoch: 39 step: 110 loss: 0.0006244892061174268\n",
            "epoch: 39 step: 120 loss: 0.0010219392639645765\n",
            "epoch: 39 step: 130 loss: 0.0013205940595457369\n",
            "epoch: 39 step: 140 loss: 0.0009722066597127876\n",
            "epoch: 39 step: 150 loss: 0.0013034535489423048\n",
            "epoch: 39 step: 160 loss: 0.001115158090686211\n",
            "epoch: 39 step: 170 loss: 0.0013912427189908452\n",
            "epoch: 39 step: 180 loss: 0.0008275051800914655\n",
            "epoch: 39 finished! loss: 0.0008275051800914655 learning_rate: [0.0004]\n",
            "epoch: 40 step: 10 loss: 0.0008747557854842174\n",
            "epoch: 40 step: 20 loss: 0.0010757391575538476\n",
            "epoch: 40 step: 30 loss: 0.0006657994147752123\n",
            "epoch: 40 step: 40 loss: 0.0007814407531313458\n",
            "epoch: 40 step: 50 loss: 0.000787158917546272\n",
            "epoch: 40 step: 60 loss: 0.0013390179527291005\n",
            "epoch: 40 step: 70 loss: 0.0010728176223388957\n",
            "epoch: 40 step: 80 loss: 0.0010331964520902941\n",
            "epoch: 40 step: 90 loss: 0.0009113064607018411\n",
            "epoch: 40 step: 100 loss: 0.0018470029494541981\n",
            "epoch: 40 step: 110 loss: 0.0014684039344192896\n",
            "epoch: 40 step: 120 loss: 0.00092751684469443\n",
            "epoch: 40 step: 130 loss: 0.0023000587075729583\n",
            "epoch: 40 step: 140 loss: 0.0012745438221472966\n",
            "epoch: 40 step: 150 loss: 0.0016542692039494518\n",
            "epoch: 40 step: 160 loss: 0.001654572453738988\n",
            "epoch: 40 step: 170 loss: 0.001432542319988849\n",
            "epoch: 40 step: 180 loss: 0.001767044979449452\n",
            "epoch: 40 finished! loss: 0.001767044979449452 learning_rate: [0.0004]\n",
            "epoch: 41 step: 10 loss: 0.0013409645214991698\n",
            "epoch: 41 step: 20 loss: 0.0009335204332247535\n",
            "epoch: 41 step: 30 loss: 0.0014145107778480927\n",
            "epoch: 41 step: 40 loss: 0.0007337854208917284\n",
            "epoch: 41 step: 50 loss: 0.0017361474926998834\n",
            "epoch: 41 step: 60 loss: 0.0010458999518618672\n",
            "epoch: 41 step: 70 loss: 0.0011394858589469402\n",
            "epoch: 41 step: 80 loss: 0.0010262171106849031\n",
            "epoch: 41 step: 90 loss: 0.0009656764635975202\n",
            "epoch: 41 step: 100 loss: 0.0009765748886238048\n",
            "epoch: 41 step: 110 loss: 0.0017414863478893406\n",
            "epoch: 41 step: 120 loss: 0.0020964632591160314\n",
            "epoch: 41 step: 130 loss: 0.002011661311086155\n",
            "epoch: 41 step: 140 loss: 0.0011242906330234902\n",
            "epoch: 41 step: 150 loss: 0.0025098889298012777\n",
            "epoch: 41 step: 160 loss: 0.001448644414002452\n",
            "epoch: 41 step: 170 loss: 0.0017824871573548287\n",
            "epoch: 41 step: 180 loss: 0.0010531136558144618\n",
            "epoch: 41 finished! loss: 0.0010531136558144618 learning_rate: [0.0004]\n",
            "epoch: 42 step: 10 loss: 0.0011041125389836912\n",
            "epoch: 42 step: 20 loss: 0.0009725501009181027\n",
            "epoch: 42 step: 30 loss: 0.0017231410181023014\n",
            "epoch: 42 step: 40 loss: 0.0008414723334210937\n",
            "epoch: 42 step: 50 loss: 0.0023017911767093434\n",
            "epoch: 42 step: 60 loss: 0.0022373369238194974\n",
            "epoch: 42 step: 70 loss: 0.0014082012163177536\n",
            "epoch: 42 step: 80 loss: 0.0008928782099153407\n",
            "epoch: 42 step: 90 loss: 0.0010356973094239835\n",
            "epoch: 42 step: 100 loss: 0.0015818009609580462\n",
            "epoch: 42 step: 110 loss: 0.0015366828996828828\n",
            "epoch: 42 step: 120 loss: 0.001203120965762829\n",
            "epoch: 42 step: 130 loss: 0.0010503561850434045\n",
            "epoch: 42 step: 140 loss: 0.0016002448786430023\n",
            "epoch: 42 step: 150 loss: 0.0011311841981885306\n",
            "epoch: 42 step: 160 loss: 0.0013438388622204992\n",
            "epoch: 42 step: 170 loss: 0.0008315831077201229\n",
            "epoch: 42 step: 180 loss: 0.002191209112012337\n",
            "epoch: 42 finished! loss: 0.002191209112012337 learning_rate: [0.0004]\n",
            "epoch: 43 step: 10 loss: 0.0010311879078355873\n",
            "epoch: 43 step: 20 loss: 0.0009161550368504041\n",
            "epoch: 43 step: 30 loss: 0.002814719347072779\n",
            "epoch: 43 step: 40 loss: 0.0017545341868935934\n",
            "epoch: 43 step: 50 loss: 0.0009318202958552631\n",
            "epoch: 43 step: 60 loss: 0.0010501760640981676\n",
            "epoch: 43 step: 70 loss: 0.0020271141492344768\n",
            "epoch: 43 step: 80 loss: 0.0015879935005428183\n",
            "epoch: 43 step: 90 loss: 0.0009100448754244419\n",
            "epoch: 43 step: 100 loss: 0.00128624026139394\n",
            "epoch: 43 step: 110 loss: 0.0015789212709667604\n",
            "epoch: 43 step: 120 loss: 0.0012759355864857247\n",
            "epoch: 43 step: 130 loss: 0.000616495957749127\n",
            "epoch: 43 step: 140 loss: 0.0010428383452863524\n",
            "epoch: 43 step: 150 loss: 0.001625532248653337\n",
            "epoch: 43 step: 160 loss: 0.0009156077225227103\n",
            "epoch: 43 step: 170 loss: 0.0022476167540175485\n",
            "epoch: 43 step: 180 loss: 0.0012615801153556827\n",
            "epoch: 43 finished! loss: 0.0012615801153556827 learning_rate: [0.0004]\n",
            "epoch: 44 step: 10 loss: 0.0011761444946201067\n",
            "epoch: 44 step: 20 loss: 0.0007944241450220575\n",
            "epoch: 44 step: 30 loss: 0.0015877011943661958\n",
            "epoch: 44 step: 40 loss: 0.0006372951084684272\n",
            "epoch: 44 step: 50 loss: 0.0009769657417436034\n",
            "epoch: 44 step: 60 loss: 0.0005535434060099595\n",
            "epoch: 44 step: 70 loss: 0.0014102420253503318\n",
            "epoch: 44 step: 80 loss: 0.0006948099745551518\n",
            "epoch: 44 step: 90 loss: 0.0010800019993513685\n",
            "epoch: 44 step: 100 loss: 0.0010320640957670278\n",
            "epoch: 44 step: 110 loss: 0.0018942239402512846\n",
            "epoch: 44 step: 120 loss: 0.0016381255332126926\n",
            "epoch: 44 step: 130 loss: 0.001519658048285957\n",
            "epoch: 44 step: 140 loss: 0.0010695296114662967\n",
            "epoch: 44 step: 150 loss: 0.0009193381671995765\n",
            "epoch: 44 step: 160 loss: 0.000807294975081693\n",
            "epoch: 44 step: 170 loss: 0.0011909984092651634\n",
            "epoch: 44 step: 180 loss: 0.0014466982375196985\n",
            "epoch: 44 finished! loss: 0.0014466982375196985 learning_rate: [0.0004]\n",
            "epoch: 45 step: 10 loss: 0.0009613288110233329\n",
            "epoch: 45 step: 20 loss: 0.000680324925413052\n",
            "epoch: 45 step: 30 loss: 0.0006669075368891932\n",
            "epoch: 45 step: 40 loss: 0.0016261281657286393\n",
            "epoch: 45 step: 50 loss: 0.0011107373449242887\n",
            "epoch: 45 step: 60 loss: 0.001303091485955006\n",
            "epoch: 45 step: 70 loss: 0.0012474357658230737\n",
            "epoch: 45 step: 80 loss: 0.0007228024430151424\n",
            "epoch: 45 step: 90 loss: 0.0013667908195088777\n",
            "epoch: 45 step: 100 loss: 0.000665381967052785\n",
            "epoch: 45 step: 110 loss: 0.0007917400866076355\n",
            "epoch: 45 step: 120 loss: 0.0009006256936901077\n",
            "epoch: 45 step: 130 loss: 0.0008381260663270161\n",
            "epoch: 45 step: 140 loss: 0.0011160221439890346\n",
            "epoch: 45 step: 150 loss: 0.0021409060170447807\n",
            "epoch: 45 step: 160 loss: 0.0016875704680286592\n",
            "epoch: 45 step: 170 loss: 0.0009008316508245429\n",
            "epoch: 45 step: 180 loss: 0.0008197593922785838\n",
            "epoch: 45 finished! loss: 0.0008197593922785838 learning_rate: [0.0004]\n",
            "epoch: 46 step: 10 loss: 0.0005958261188652318\n",
            "epoch: 46 step: 20 loss: 0.00092354583626589\n",
            "epoch: 46 step: 30 loss: 0.0010159642153177374\n",
            "epoch: 46 step: 40 loss: 0.000889212616737359\n",
            "epoch: 46 step: 50 loss: 0.0008140376512050611\n",
            "epoch: 46 step: 60 loss: 0.001444919678345739\n",
            "epoch: 46 step: 70 loss: 0.0011804356329281986\n",
            "epoch: 46 step: 80 loss: 0.0006746413585032253\n",
            "epoch: 46 step: 90 loss: 0.001008457051915626\n",
            "epoch: 46 step: 100 loss: 0.0012319885310180488\n",
            "epoch: 46 step: 110 loss: 0.0013439934587115819\n",
            "epoch: 46 step: 120 loss: 0.0011616363858094091\n",
            "epoch: 46 step: 130 loss: 0.0005557716686663317\n",
            "epoch: 46 step: 140 loss: 0.001347315219716836\n",
            "epoch: 46 step: 150 loss: 0.0013863127051959754\n",
            "epoch: 46 step: 160 loss: 0.0009776793941735581\n",
            "epoch: 46 step: 170 loss: 0.0008276878078742851\n",
            "epoch: 46 step: 180 loss: 0.0012708389447054025\n",
            "epoch: 46 finished! loss: 0.0012708389447054025 learning_rate: [0.0004]\n",
            "epoch: 47 step: 10 loss: 0.0009109678631493127\n",
            "epoch: 47 step: 20 loss: 0.0012123225347569532\n",
            "epoch: 47 step: 30 loss: 0.0008703233342797379\n",
            "epoch: 47 step: 40 loss: 0.0023437587466064434\n",
            "epoch: 47 step: 50 loss: 0.0012553553423475911\n",
            "epoch: 47 step: 60 loss: 0.002455905392880513\n",
            "epoch: 47 step: 70 loss: 0.0017569266329485306\n",
            "epoch: 47 step: 80 loss: 0.0024181041064124113\n",
            "epoch: 47 step: 90 loss: 0.0012944351218298394\n",
            "epoch: 47 step: 100 loss: 0.0012014538740704646\n",
            "epoch: 47 step: 110 loss: 0.0009184430976850813\n",
            "epoch: 47 step: 120 loss: 0.0020534352176292273\n",
            "epoch: 47 step: 130 loss: 0.0007805976109792604\n",
            "epoch: 47 step: 140 loss: 0.0007951785218449161\n",
            "epoch: 47 step: 150 loss: 0.0011571778308748345\n",
            "epoch: 47 step: 160 loss: 0.0009169688769801511\n",
            "epoch: 47 step: 170 loss: 0.0018245176170461545\n",
            "epoch: 47 step: 180 loss: 0.0012921601335068042\n",
            "epoch: 47 finished! loss: 0.0012921601335068042 learning_rate: [0.0004]\n",
            "epoch: 48 step: 10 loss: 0.0009362564929311277\n",
            "epoch: 48 step: 20 loss: 0.001680509363972845\n",
            "epoch: 48 step: 30 loss: 0.0012522695331928485\n",
            "epoch: 48 step: 40 loss: 0.0015092080753766317\n",
            "epoch: 48 step: 50 loss: 0.0012377820311384356\n",
            "epoch: 48 step: 60 loss: 0.0007340764259498621\n",
            "epoch: 48 step: 70 loss: 0.0011386463083379464\n",
            "epoch: 48 step: 80 loss: 0.0007235507691102636\n",
            "epoch: 48 step: 90 loss: 0.000866038203429654\n",
            "epoch: 48 step: 100 loss: 0.0010590095326070436\n",
            "epoch: 48 step: 110 loss: 0.000979628731528702\n",
            "epoch: 48 step: 120 loss: 0.0025780778342000027\n",
            "epoch: 48 step: 130 loss: 0.0012218184406608318\n",
            "epoch: 48 step: 140 loss: 0.0008489519101730985\n",
            "epoch: 48 step: 150 loss: 0.0011452249994029177\n",
            "epoch: 48 step: 160 loss: 0.0013294161157798975\n",
            "epoch: 48 step: 170 loss: 0.0012229869326111878\n",
            "epoch: 48 step: 180 loss: 0.0010120431249988119\n",
            "epoch: 48 finished! loss: 0.0010120431249988119 learning_rate: [0.0004]\n",
            "epoch: 49 step: 10 loss: 0.000779408314987676\n",
            "epoch: 49 step: 20 loss: 0.0010490908213196214\n",
            "epoch: 49 step: 30 loss: 0.0005768206645841038\n",
            "epoch: 49 step: 40 loss: 0.0012590851876880932\n",
            "epoch: 49 step: 50 loss: 0.0017714617443094912\n",
            "epoch: 49 step: 60 loss: 0.0008250201897411056\n",
            "epoch: 49 step: 70 loss: 0.0012114002993062192\n",
            "epoch: 49 step: 80 loss: 0.001997376747641787\n",
            "epoch: 49 step: 90 loss: 0.0010015989650014455\n",
            "epoch: 49 step: 100 loss: 0.0007696116940834998\n",
            "epoch: 49 step: 110 loss: 0.001287852547921824\n",
            "epoch: 49 step: 120 loss: 0.0007291492084263436\n",
            "epoch: 49 step: 130 loss: 0.0012404145420913128\n",
            "epoch: 49 step: 140 loss: 0.0038563241193231602\n",
            "epoch: 49 step: 150 loss: 0.0008321996924130262\n",
            "epoch: 49 step: 160 loss: 0.0009432582300520963\n",
            "epoch: 49 step: 170 loss: 0.0008070361273739905\n",
            "epoch: 49 step: 180 loss: 0.0016880171489674597\n",
            "epoch: 49 finished! loss: 0.0016880171489674597 learning_rate: [0.0004]\n",
            "epoch: 50 step: 10 loss: 0.0009463722125633727\n",
            "epoch: 50 step: 20 loss: 0.0008446324552812215\n",
            "epoch: 50 step: 30 loss: 0.0008203693085913138\n",
            "epoch: 50 step: 40 loss: 0.0009947375648658296\n",
            "epoch: 50 step: 50 loss: 0.000851928528520664\n",
            "epoch: 50 step: 60 loss: 0.0018323001663680164\n",
            "epoch: 50 step: 70 loss: 0.0007194667655498824\n",
            "epoch: 50 step: 80 loss: 0.0005296610903415822\n",
            "epoch: 50 step: 90 loss: 0.0011703545706315155\n",
            "epoch: 50 step: 100 loss: 0.0010829628427727876\n",
            "epoch: 50 step: 110 loss: 0.0008349954162998207\n",
            "epoch: 50 step: 120 loss: 0.0008499648001159382\n",
            "epoch: 50 step: 130 loss: 0.0008610229996442595\n",
            "epoch: 50 step: 140 loss: 0.0007658440029178749\n",
            "epoch: 50 step: 150 loss: 0.0016587586736886797\n",
            "epoch: 50 step: 160 loss: 0.00060034247499018\n",
            "epoch: 50 step: 170 loss: 0.0008093536385978497\n",
            "epoch: 50 step: 180 loss: 0.0012032207317660964\n",
            "epoch: 50 finished! loss: 0.0012032207317660964 learning_rate: [0.0004]\n",
            "epoch: 51 step: 10 loss: 0.0009567256036520939\n",
            "epoch: 51 step: 20 loss: 0.0008862885653467011\n",
            "epoch: 51 step: 30 loss: 0.000557349520572397\n",
            "epoch: 51 step: 40 loss: 0.0006075693000371517\n",
            "epoch: 51 step: 50 loss: 0.0004074529390417731\n",
            "epoch: 51 step: 60 loss: 0.0004799336154372288\n",
            "epoch: 51 step: 70 loss: 0.0006658466029181632\n",
            "epoch: 51 step: 80 loss: 0.0007524001599878042\n",
            "epoch: 51 step: 90 loss: 0.0008262939847814274\n",
            "epoch: 51 step: 100 loss: 0.0006579669957380192\n",
            "epoch: 51 step: 110 loss: 0.0010444060822297884\n",
            "epoch: 51 step: 120 loss: 0.000878025945456346\n",
            "epoch: 51 step: 130 loss: 0.0008178847991367474\n",
            "epoch: 51 step: 140 loss: 0.0007769803097469634\n",
            "epoch: 51 step: 150 loss: 0.001411255610176841\n",
            "epoch: 51 step: 160 loss: 0.0006246914441179316\n",
            "epoch: 51 step: 170 loss: 0.0009745098118974528\n",
            "epoch: 51 step: 180 loss: 0.0007062385653896022\n",
            "epoch: 51 finished! loss: 0.0007062385653896022 learning_rate: [0.0001]\n",
            "epoch: 52 step: 10 loss: 0.000585272305560734\n",
            "epoch: 52 step: 20 loss: 0.0004297601072443971\n",
            "epoch: 52 step: 30 loss: 0.0002902866149294009\n",
            "epoch: 52 step: 40 loss: 0.000405694790491047\n",
            "epoch: 52 step: 50 loss: 0.0004876597110678984\n",
            "epoch: 52 step: 60 loss: 0.0007660215051258024\n",
            "epoch: 52 step: 70 loss: 0.0006168275933257089\n",
            "epoch: 52 step: 80 loss: 0.00036432563900968277\n",
            "epoch: 52 step: 90 loss: 0.0006475554463804892\n",
            "epoch: 52 step: 100 loss: 0.0005441709671072441\n",
            "epoch: 52 step: 110 loss: 0.0005564437652394968\n",
            "epoch: 52 step: 120 loss: 0.00044435437289603436\n",
            "epoch: 52 step: 130 loss: 0.0003597091395625755\n",
            "epoch: 52 step: 140 loss: 0.0012719351406775105\n",
            "epoch: 52 step: 150 loss: 0.0007307426474101325\n",
            "epoch: 52 step: 160 loss: 0.0004907168466901123\n",
            "epoch: 52 step: 170 loss: 0.00043409525648207425\n",
            "epoch: 52 step: 180 loss: 0.0007334568426370915\n",
            "epoch: 52 finished! loss: 0.0007334568426370915 learning_rate: [0.0002]\n",
            "epoch: 53 step: 10 loss: 0.0005023777383154112\n",
            "epoch: 53 step: 20 loss: 0.0002897368225719301\n",
            "epoch: 53 step: 30 loss: 0.0003485576892628269\n",
            "epoch: 53 step: 40 loss: 0.0006080992413506607\n",
            "epoch: 53 step: 50 loss: 0.000800379192552403\n",
            "epoch: 53 step: 60 loss: 0.0002790881325242684\n",
            "epoch: 53 step: 70 loss: 0.00035431835162019683\n",
            "epoch: 53 step: 80 loss: 0.0007363573418756831\n",
            "epoch: 53 step: 90 loss: 0.0007038975520273548\n",
            "epoch: 53 step: 100 loss: 0.0005022449271924191\n",
            "epoch: 53 step: 110 loss: 0.00039015064530440504\n",
            "epoch: 53 step: 120 loss: 0.0004211255547054535\n",
            "epoch: 53 step: 130 loss: 0.0005126438602208417\n",
            "epoch: 53 step: 140 loss: 0.00042672759317137525\n",
            "epoch: 53 step: 150 loss: 0.0005425012644026106\n",
            "epoch: 53 step: 160 loss: 0.0003542479591417504\n",
            "epoch: 53 step: 170 loss: 0.0009285313546075152\n",
            "epoch: 53 step: 180 loss: 0.000290367682589312\n",
            "epoch: 53 finished! loss: 0.000290367682589312 learning_rate: [0.0002]\n",
            "epoch: 54 step: 10 loss: 0.0004116993863944343\n",
            "epoch: 54 step: 20 loss: 0.0004906178914916193\n",
            "epoch: 54 step: 30 loss: 0.0004676872590062262\n",
            "epoch: 54 step: 40 loss: 0.0008320222768959206\n",
            "epoch: 54 step: 50 loss: 0.0006433832183386852\n",
            "epoch: 54 step: 60 loss: 0.0009815566436670252\n",
            "epoch: 54 step: 70 loss: 0.0004152633495784549\n",
            "epoch: 54 step: 80 loss: 0.0005317158361314181\n",
            "epoch: 54 step: 90 loss: 0.0003508224358877016\n",
            "epoch: 54 step: 100 loss: 0.0007868600356116771\n",
            "epoch: 54 step: 110 loss: 0.00038861108437290013\n",
            "epoch: 54 step: 120 loss: 0.0007102454629160979\n",
            "epoch: 54 step: 130 loss: 0.0006081727661062584\n",
            "epoch: 54 step: 140 loss: 0.0005594364659497119\n",
            "epoch: 54 step: 150 loss: 0.00026540440637354854\n",
            "epoch: 54 step: 160 loss: 0.00037228442695926803\n",
            "epoch: 54 step: 170 loss: 0.0005283171881820483\n",
            "epoch: 54 step: 180 loss: 0.0005107897259023781\n",
            "epoch: 54 finished! loss: 0.0005107897259023781 learning_rate: [0.0002]\n",
            "epoch: 55 step: 10 loss: 0.0003806319454532979\n",
            "epoch: 55 step: 20 loss: 0.0006169730038931073\n",
            "epoch: 55 step: 30 loss: 0.0008348210519456981\n",
            "epoch: 55 step: 40 loss: 0.00040916047842820763\n",
            "epoch: 55 step: 50 loss: 0.00041615499194415075\n",
            "epoch: 55 step: 60 loss: 0.00018901878220090704\n",
            "epoch: 55 step: 70 loss: 0.00046325753784593057\n",
            "epoch: 55 step: 80 loss: 0.000939806536517208\n",
            "epoch: 55 step: 90 loss: 0.0006076749367675699\n",
            "epoch: 55 step: 100 loss: 0.0002953269136410738\n",
            "epoch: 55 step: 110 loss: 0.00031218175291858964\n",
            "epoch: 55 step: 120 loss: 0.0007204226010970848\n",
            "epoch: 55 step: 130 loss: 0.00044709512819246657\n",
            "epoch: 55 step: 140 loss: 0.0003994338961699563\n",
            "epoch: 55 step: 150 loss: 0.0005779118129035141\n",
            "epoch: 55 step: 160 loss: 0.0006062041295586402\n",
            "epoch: 55 step: 170 loss: 0.0005213538330961844\n",
            "epoch: 55 step: 180 loss: 0.00045483708911653987\n",
            "epoch: 55 finished! loss: 0.00045483708911653987 learning_rate: [0.0002]\n",
            "epoch: 56 step: 10 loss: 0.00033281868853554685\n",
            "epoch: 56 step: 20 loss: 0.00064140919896699\n",
            "epoch: 56 step: 30 loss: 0.0007257211424095794\n",
            "epoch: 56 step: 40 loss: 0.0004112788741609973\n",
            "epoch: 56 step: 50 loss: 0.00024381858340568593\n",
            "epoch: 56 step: 60 loss: 0.0005599587368710365\n",
            "epoch: 56 step: 70 loss: 0.000306944303132893\n",
            "epoch: 56 step: 80 loss: 0.000712437835452707\n",
            "epoch: 56 step: 90 loss: 0.0005370457936469713\n",
            "epoch: 56 step: 100 loss: 0.0005064408832681643\n",
            "epoch: 56 step: 110 loss: 0.0004413177240575322\n",
            "epoch: 56 step: 120 loss: 0.0005791348807920746\n",
            "epoch: 56 step: 130 loss: 0.0003186516753546566\n",
            "epoch: 56 step: 140 loss: 0.0004178260267786809\n",
            "epoch: 56 step: 150 loss: 0.0005326787741344351\n",
            "epoch: 56 step: 160 loss: 0.00030743862710989363\n",
            "epoch: 56 step: 170 loss: 0.0005967549850743137\n",
            "epoch: 56 step: 180 loss: 0.0005587493509820801\n",
            "epoch: 56 finished! loss: 0.0005587493509820801 learning_rate: [0.0002]\n",
            "epoch: 57 step: 10 loss: 0.0004637693591296728\n",
            "epoch: 57 step: 20 loss: 0.00032320270522884096\n",
            "epoch: 57 step: 30 loss: 0.00028528300107147147\n",
            "epoch: 57 step: 40 loss: 0.0004777527680656908\n",
            "epoch: 57 step: 50 loss: 0.000511392275937423\n",
            "epoch: 57 step: 60 loss: 0.00043173928644448857\n",
            "epoch: 57 step: 70 loss: 0.00031804296185318715\n",
            "epoch: 57 step: 80 loss: 0.0009543039619704759\n",
            "epoch: 57 step: 90 loss: 0.00035943491059264746\n",
            "epoch: 57 step: 100 loss: 0.0005263466468713642\n",
            "epoch: 57 step: 110 loss: 0.0005608179928666526\n",
            "epoch: 57 step: 120 loss: 0.00039935129288181016\n",
            "epoch: 57 step: 130 loss: 0.0006026565805550656\n",
            "epoch: 57 step: 140 loss: 0.00043622993578553223\n",
            "epoch: 57 step: 150 loss: 0.0005623438513910479\n",
            "epoch: 57 step: 160 loss: 0.0003596700725247471\n",
            "epoch: 57 step: 170 loss: 0.0002655528400694812\n",
            "epoch: 57 step: 180 loss: 0.0006094162588644309\n",
            "epoch: 57 finished! loss: 0.0006094162588644309 learning_rate: [0.0002]\n",
            "epoch: 58 step: 10 loss: 0.00032153956943588866\n",
            "epoch: 58 step: 20 loss: 0.0004149073725355293\n",
            "epoch: 58 step: 30 loss: 0.0004729419188851284\n",
            "epoch: 58 step: 40 loss: 0.00023713225020289823\n",
            "epoch: 58 step: 50 loss: 0.0002724360070603918\n",
            "epoch: 58 step: 60 loss: 0.0005809974892054856\n",
            "epoch: 58 step: 70 loss: 0.000598955075688609\n",
            "epoch: 58 step: 80 loss: 0.00028086527444641697\n",
            "epoch: 58 step: 90 loss: 0.000842573265403646\n",
            "epoch: 58 step: 100 loss: 0.0005107458051596033\n",
            "epoch: 58 step: 110 loss: 0.0006157422187008905\n",
            "epoch: 58 step: 120 loss: 0.0003297775219373564\n",
            "epoch: 58 step: 130 loss: 0.00024252062733149892\n",
            "epoch: 58 step: 140 loss: 0.0003907677187808066\n",
            "epoch: 58 step: 150 loss: 0.00039163177857017576\n",
            "epoch: 58 step: 160 loss: 0.0008019077255380313\n",
            "epoch: 58 step: 170 loss: 0.00044055569989142593\n",
            "epoch: 58 step: 180 loss: 0.0003651583307753056\n",
            "epoch: 58 finished! loss: 0.0003651583307753056 learning_rate: [0.0002]\n",
            "epoch: 59 step: 10 loss: 0.0002832208462320285\n",
            "epoch: 59 step: 20 loss: 0.0007176554544070696\n",
            "epoch: 59 step: 30 loss: 0.000261244707012187\n",
            "epoch: 59 step: 40 loss: 0.0004186032130468078\n",
            "epoch: 59 step: 50 loss: 0.0003328462285860733\n",
            "epoch: 59 step: 60 loss: 0.00032370832844629227\n",
            "epoch: 59 step: 70 loss: 0.00041591016884493545\n",
            "epoch: 59 step: 80 loss: 0.0005598085414054857\n",
            "epoch: 59 step: 90 loss: 0.0005533776048823937\n",
            "epoch: 59 step: 100 loss: 0.0003553843543488295\n",
            "epoch: 59 step: 110 loss: 0.0005564495428583323\n",
            "epoch: 59 step: 120 loss: 0.0004021562638557314\n",
            "epoch: 59 step: 130 loss: 0.000506299496963191\n",
            "epoch: 59 step: 140 loss: 0.00039586144827966286\n",
            "epoch: 59 step: 150 loss: 0.0003224351978178432\n",
            "epoch: 59 step: 160 loss: 0.0004142433037448994\n",
            "epoch: 59 step: 170 loss: 0.0005702834463857177\n",
            "epoch: 59 step: 180 loss: 0.0005383850985828791\n",
            "epoch: 59 finished! loss: 0.0005383850985828791 learning_rate: [0.0002]\n",
            "epoch: 60 step: 10 loss: 0.00034064479317901083\n",
            "epoch: 60 step: 20 loss: 0.0003754807321696069\n",
            "epoch: 60 step: 30 loss: 0.0003678491476052119\n",
            "epoch: 60 step: 40 loss: 0.00040602299416317417\n",
            "epoch: 60 step: 50 loss: 0.0003850660532155769\n",
            "epoch: 60 step: 60 loss: 0.00039767687088789036\n",
            "epoch: 60 step: 70 loss: 0.00044174025082553724\n",
            "epoch: 60 step: 80 loss: 0.0005241131792892007\n",
            "epoch: 60 step: 90 loss: 0.0003867865100689367\n",
            "epoch: 60 step: 100 loss: 0.000568534892224755\n",
            "epoch: 60 step: 110 loss: 0.000604631420605123\n",
            "epoch: 60 step: 120 loss: 0.0006357485501567487\n",
            "epoch: 60 step: 130 loss: 0.00036332181510876434\n",
            "epoch: 60 step: 140 loss: 0.0004363797062505514\n",
            "epoch: 60 step: 150 loss: 0.00029878910888820085\n",
            "epoch: 60 step: 160 loss: 0.0003002993796054795\n",
            "epoch: 60 step: 170 loss: 0.0007205921034858558\n",
            "epoch: 60 step: 180 loss: 0.0007058917602198959\n",
            "epoch: 60 finished! loss: 0.0007058917602198959 learning_rate: [0.0002]\n",
            "epoch: 61 step: 10 loss: 0.0004435263518349428\n",
            "epoch: 61 step: 20 loss: 0.0003708334728270948\n",
            "epoch: 61 step: 30 loss: 0.0003126108660165943\n",
            "epoch: 61 step: 40 loss: 0.0005561190217330118\n",
            "epoch: 61 step: 50 loss: 0.00031203669090920416\n",
            "epoch: 61 step: 60 loss: 0.0003470724236190689\n",
            "epoch: 61 step: 70 loss: 0.0007550042047902759\n",
            "epoch: 61 step: 80 loss: 0.0005811547959839816\n",
            "epoch: 61 step: 90 loss: 0.00026403135919701753\n",
            "epoch: 61 step: 100 loss: 0.0005981503486732654\n",
            "epoch: 61 step: 110 loss: 0.00039849487599638594\n",
            "epoch: 61 step: 120 loss: 0.0006187788605481645\n",
            "epoch: 61 step: 130 loss: 0.0005242210939212561\n",
            "epoch: 61 step: 140 loss: 0.000599197043158019\n",
            "epoch: 61 step: 150 loss: 0.0006056813195373751\n",
            "epoch: 61 step: 160 loss: 0.00028616671335567366\n",
            "epoch: 61 step: 170 loss: 0.0005631446191879885\n",
            "epoch: 61 step: 180 loss: 0.0005424966723321452\n",
            "epoch: 61 finished! loss: 0.0005424966723321452 learning_rate: [0.0002]\n",
            "epoch: 62 step: 10 loss: 0.0003786951101984737\n",
            "epoch: 62 step: 20 loss: 0.001035701088998655\n",
            "epoch: 62 step: 30 loss: 0.000927055745279134\n",
            "epoch: 62 step: 40 loss: 0.0008457084725707135\n",
            "epoch: 62 step: 50 loss: 0.000759224950903427\n",
            "epoch: 62 step: 60 loss: 0.0011393790739423356\n",
            "epoch: 62 step: 70 loss: 0.0013095164543501772\n",
            "epoch: 62 step: 80 loss: 0.0007213678880950648\n",
            "epoch: 62 step: 90 loss: 0.0006358931216246694\n",
            "epoch: 62 step: 100 loss: 0.0007744734786705287\n",
            "epoch: 62 step: 110 loss: 0.0005206303111948176\n",
            "epoch: 62 step: 120 loss: 0.0007180043211826942\n",
            "epoch: 62 step: 130 loss: 0.000548586234369188\n",
            "epoch: 62 step: 140 loss: 0.0005881486582727203\n",
            "epoch: 62 step: 150 loss: 0.0004609825344665925\n",
            "epoch: 62 step: 160 loss: 0.0005550458112037321\n",
            "epoch: 62 step: 170 loss: 0.0006480279395290591\n",
            "epoch: 62 step: 180 loss: 0.0003515666759947037\n",
            "epoch: 62 finished! loss: 0.0003515666759947037 learning_rate: [0.0002]\n",
            "epoch: 63 step: 10 loss: 0.00032026144672411925\n",
            "epoch: 63 step: 20 loss: 0.00086667232089214\n",
            "epoch: 63 step: 30 loss: 0.0005080776150128842\n",
            "epoch: 63 step: 40 loss: 0.00037083179982233885\n",
            "epoch: 63 step: 50 loss: 0.00037381509648044196\n",
            "epoch: 63 step: 60 loss: 0.00027346206700383307\n",
            "epoch: 63 step: 70 loss: 0.0004808418791811882\n",
            "epoch: 63 step: 80 loss: 0.00027348569116155964\n",
            "epoch: 63 step: 90 loss: 0.0003099994403965441\n",
            "epoch: 63 step: 100 loss: 0.00037514871499512784\n",
            "epoch: 63 step: 110 loss: 0.0008654454648679816\n",
            "epoch: 63 step: 120 loss: 0.0005706471546290975\n",
            "epoch: 63 step: 130 loss: 0.0004509893378976127\n",
            "epoch: 63 step: 140 loss: 0.00036184532364925704\n",
            "epoch: 63 step: 150 loss: 0.0008048382441798601\n",
            "epoch: 63 step: 160 loss: 0.0007812663145269633\n",
            "epoch: 63 step: 170 loss: 0.000812253842704962\n",
            "epoch: 63 step: 180 loss: 0.0005976867884489882\n",
            "epoch: 63 finished! loss: 0.0005976867884489882 learning_rate: [0.0002]\n",
            "epoch: 64 step: 10 loss: 0.0007358951152490296\n",
            "epoch: 64 step: 20 loss: 0.0006802999575508347\n",
            "epoch: 64 step: 30 loss: 0.0021859033463245197\n",
            "epoch: 64 step: 40 loss: 0.00040025951327814745\n",
            "epoch: 64 step: 50 loss: 0.0004356501153568991\n",
            "epoch: 64 step: 60 loss: 0.0009359244851161088\n",
            "epoch: 64 step: 70 loss: 0.0003983502088693896\n",
            "epoch: 64 step: 80 loss: 0.00027200400314542083\n",
            "epoch: 64 step: 90 loss: 0.00035980988732192963\n",
            "epoch: 64 step: 100 loss: 0.0004397514939938448\n",
            "epoch: 64 step: 110 loss: 0.0009174502584124594\n",
            "epoch: 64 step: 120 loss: 0.0014086304759731345\n",
            "epoch: 64 step: 130 loss: 0.0003135749072506201\n",
            "epoch: 64 step: 140 loss: 0.000668944408792198\n",
            "epoch: 64 step: 150 loss: 0.0005313005716456531\n",
            "epoch: 64 step: 160 loss: 0.0004886868801891721\n",
            "epoch: 64 step: 170 loss: 0.00037876916823469955\n",
            "epoch: 64 step: 180 loss: 0.00040728346674685366\n",
            "epoch: 64 finished! loss: 0.00040728346674685366 learning_rate: [0.0002]\n",
            "epoch: 65 step: 10 loss: 0.00045900598536137514\n",
            "epoch: 65 step: 20 loss: 0.0012337379878114862\n",
            "epoch: 65 step: 30 loss: 0.0003140272254778198\n",
            "epoch: 65 step: 40 loss: 0.0007096927829249649\n",
            "epoch: 65 step: 50 loss: 0.0004842096958797021\n",
            "epoch: 65 step: 60 loss: 0.00039692097122849855\n",
            "epoch: 65 step: 70 loss: 0.0005909851101188219\n",
            "epoch: 65 step: 80 loss: 0.00053304025289212\n",
            "epoch: 65 step: 90 loss: 0.0004531592473611238\n",
            "epoch: 65 step: 100 loss: 0.0002792844494718692\n",
            "epoch: 65 step: 110 loss: 0.001011686680080205\n",
            "epoch: 65 step: 120 loss: 0.00030893956595792017\n",
            "epoch: 65 step: 130 loss: 0.000651358877697736\n",
            "epoch: 65 step: 140 loss: 0.0010732556789406333\n",
            "epoch: 65 step: 150 loss: 0.0004948182653260625\n",
            "epoch: 65 step: 160 loss: 0.0005626530122802485\n",
            "epoch: 65 step: 170 loss: 0.0008871039169191811\n",
            "epoch: 65 step: 180 loss: 0.0005262718752369884\n",
            "epoch: 65 finished! loss: 0.0005262718752369884 learning_rate: [0.0002]\n",
            "epoch: 66 step: 10 loss: 0.00047571388140291696\n",
            "epoch: 66 step: 20 loss: 0.00025367532822783383\n",
            "epoch: 66 step: 30 loss: 0.0005822803758998473\n",
            "epoch: 66 step: 40 loss: 0.00046648496653142247\n",
            "epoch: 66 step: 50 loss: 0.0006972831065508925\n",
            "epoch: 66 step: 60 loss: 0.00032923060568582405\n",
            "epoch: 66 step: 70 loss: 0.0007281099772216411\n",
            "epoch: 66 step: 80 loss: 0.0005356988703446113\n",
            "epoch: 66 step: 90 loss: 0.0007875109844998802\n",
            "epoch: 66 step: 100 loss: 0.00040329708295386844\n",
            "epoch: 66 step: 110 loss: 0.00043474214897471153\n",
            "epoch: 66 step: 120 loss: 0.00023477416367074022\n",
            "epoch: 66 step: 130 loss: 0.0006485452963266157\n",
            "epoch: 66 step: 140 loss: 0.000532532038210486\n",
            "epoch: 66 step: 150 loss: 0.0012282752603932161\n",
            "epoch: 66 step: 160 loss: 0.0002705753530223497\n",
            "epoch: 66 step: 170 loss: 0.000656859754686785\n",
            "epoch: 66 step: 180 loss: 0.00035894073894045687\n",
            "epoch: 66 finished! loss: 0.00035894073894045687 learning_rate: [0.0002]\n",
            "epoch: 67 step: 10 loss: 0.0002815897576756954\n",
            "epoch: 67 step: 20 loss: 0.0006060521374347068\n",
            "epoch: 67 step: 30 loss: 0.0005842272791069044\n",
            "epoch: 67 step: 40 loss: 0.0008932652180742449\n",
            "epoch: 67 step: 50 loss: 0.0006478414210584868\n",
            "epoch: 67 step: 60 loss: 0.00028682625393267557\n",
            "epoch: 67 step: 70 loss: 0.00033453788247743904\n",
            "epoch: 67 step: 80 loss: 0.0005125073944200647\n",
            "epoch: 67 step: 90 loss: 0.0004387648575407767\n",
            "epoch: 67 step: 100 loss: 0.00024033378740961865\n",
            "epoch: 67 step: 110 loss: 0.0013281398900672743\n",
            "epoch: 67 step: 120 loss: 0.0002532383206243815\n",
            "epoch: 67 step: 130 loss: 0.000251491382034348\n",
            "epoch: 67 step: 140 loss: 0.00050713902306183\n",
            "epoch: 67 step: 150 loss: 0.0003704994023388751\n",
            "epoch: 67 step: 160 loss: 0.0007897885503454488\n",
            "epoch: 67 step: 170 loss: 0.00039022346020049943\n",
            "epoch: 67 step: 180 loss: 0.0005111177166096145\n",
            "epoch: 67 finished! loss: 0.0005111177166096145 learning_rate: [0.0002]\n",
            "epoch: 68 step: 10 loss: 0.0004641693649402076\n",
            "epoch: 68 step: 20 loss: 0.0003863970848037508\n",
            "epoch: 68 step: 30 loss: 0.0003676742629867708\n",
            "epoch: 68 step: 40 loss: 0.0004709301770909014\n",
            "epoch: 68 step: 50 loss: 0.0003539286010504953\n",
            "epoch: 68 step: 60 loss: 0.0004199665138652238\n",
            "epoch: 68 step: 70 loss: 0.0003176807648954543\n",
            "epoch: 68 step: 80 loss: 0.00037048547706725553\n",
            "epoch: 68 step: 90 loss: 0.0001660554478310803\n",
            "epoch: 68 step: 100 loss: 0.00042931353351134666\n",
            "epoch: 68 step: 110 loss: 0.0004204196044505447\n",
            "epoch: 68 step: 120 loss: 0.0005277631595175343\n",
            "epoch: 68 step: 130 loss: 0.0003688797515493854\n",
            "epoch: 68 step: 140 loss: 0.0002488123417746531\n",
            "epoch: 68 step: 150 loss: 0.00034488100179198215\n",
            "epoch: 68 step: 160 loss: 0.0003045792450019459\n",
            "epoch: 68 step: 170 loss: 0.0003351995473423462\n",
            "epoch: 68 step: 180 loss: 0.00020027064202679954\n",
            "epoch: 68 finished! loss: 0.00020027064202679954 learning_rate: [0.0002]\n",
            "epoch: 69 step: 10 loss: 0.0003153116129008701\n",
            "epoch: 69 step: 20 loss: 0.0004902389259738219\n",
            "epoch: 69 step: 30 loss: 0.0005085774095925627\n",
            "epoch: 69 step: 40 loss: 0.0004458365180182851\n",
            "epoch: 69 step: 50 loss: 0.0002922319212360598\n",
            "epoch: 69 step: 60 loss: 0.0003543808799824144\n",
            "epoch: 69 step: 70 loss: 0.0002559666651610328\n",
            "epoch: 69 step: 80 loss: 0.000789721505600832\n",
            "epoch: 69 step: 90 loss: 0.0005027864181592831\n",
            "epoch: 69 step: 100 loss: 0.00036052473951368296\n",
            "epoch: 69 step: 110 loss: 0.0004442456130695138\n",
            "epoch: 69 step: 120 loss: 0.00035996224909831806\n",
            "epoch: 69 step: 130 loss: 0.0005467815945707498\n",
            "epoch: 69 step: 140 loss: 0.0006831658992281455\n",
            "epoch: 69 step: 150 loss: 0.0005232360938666097\n",
            "epoch: 69 step: 160 loss: 0.0002262725293999679\n",
            "epoch: 69 step: 170 loss: 0.00041236075309514274\n",
            "epoch: 69 step: 180 loss: 0.0003485762858155017\n",
            "epoch: 69 finished! loss: 0.0003485762858155017 learning_rate: [0.0002]\n",
            "epoch: 70 step: 10 loss: 0.0003306183270926692\n",
            "epoch: 70 step: 20 loss: 0.0004782292330203448\n",
            "epoch: 70 step: 30 loss: 0.0004555424562508519\n",
            "epoch: 70 step: 40 loss: 0.0006105795337004227\n",
            "epoch: 70 step: 50 loss: 0.0004017352038544053\n",
            "epoch: 70 step: 60 loss: 0.000512034672285591\n",
            "epoch: 70 step: 70 loss: 0.00029187809628078074\n",
            "epoch: 70 step: 80 loss: 0.00032969980752864554\n",
            "epoch: 70 step: 90 loss: 0.00043472726596715465\n",
            "epoch: 70 step: 100 loss: 0.00026297761554399513\n",
            "epoch: 70 step: 110 loss: 0.001171239737865303\n",
            "epoch: 70 step: 120 loss: 0.0007791013724033456\n",
            "epoch: 70 step: 130 loss: 0.0002740961395701075\n",
            "epoch: 70 step: 140 loss: 0.0006119317679625458\n",
            "epoch: 70 step: 150 loss: 0.0003143321244463834\n",
            "epoch: 70 step: 160 loss: 0.0014170102665474792\n",
            "epoch: 70 step: 170 loss: 0.00047581629888273665\n",
            "epoch: 70 step: 180 loss: 0.0006775527097528992\n",
            "epoch: 70 finished! loss: 0.0006775527097528992 learning_rate: [0.0002]\n",
            "epoch: 71 step: 10 loss: 0.00032577172326003285\n",
            "epoch: 71 step: 20 loss: 0.0007043558334635818\n",
            "epoch: 71 step: 30 loss: 0.00020253314907055185\n",
            "epoch: 71 step: 40 loss: 0.0003713372610923061\n",
            "epoch: 71 step: 50 loss: 0.0005027338537432831\n",
            "epoch: 71 step: 60 loss: 0.0004479323870303034\n",
            "epoch: 71 step: 70 loss: 0.0005146333449192842\n",
            "epoch: 71 step: 80 loss: 0.00044547563903937364\n",
            "epoch: 71 step: 90 loss: 0.0006035382169476276\n",
            "epoch: 71 step: 100 loss: 0.0001722033501677076\n",
            "epoch: 71 step: 110 loss: 0.0005404100111189944\n",
            "epoch: 71 step: 120 loss: 0.00044064143188301115\n",
            "epoch: 71 step: 130 loss: 0.00018573531876998664\n",
            "epoch: 71 step: 140 loss: 0.0003216186907590328\n",
            "epoch: 71 step: 150 loss: 0.0005603749987100235\n",
            "epoch: 71 step: 160 loss: 0.00021005585782220713\n",
            "epoch: 71 step: 170 loss: 0.00025519276276033477\n",
            "epoch: 71 step: 180 loss: 0.000747527089830558\n",
            "epoch: 71 finished! loss: 0.000747527089830558 learning_rate: [5e-05]\n",
            "epoch: 72 step: 10 loss: 0.0006251224508528765\n",
            "epoch: 72 step: 20 loss: 0.0002479210181988919\n",
            "epoch: 72 step: 30 loss: 0.0007489853708217164\n",
            "epoch: 72 step: 40 loss: 0.0006397915435944161\n",
            "epoch: 72 step: 50 loss: 0.0006116661123035786\n",
            "epoch: 72 step: 60 loss: 0.0005295640752935763\n",
            "epoch: 72 step: 70 loss: 0.00037565305779532787\n",
            "epoch: 72 step: 80 loss: 0.000284767458284015\n",
            "epoch: 72 step: 90 loss: 0.000276512526708576\n",
            "epoch: 72 step: 100 loss: 0.0004581272484012092\n",
            "epoch: 72 step: 110 loss: 0.00021620617306391783\n",
            "epoch: 72 step: 120 loss: 0.00044769734149986424\n",
            "epoch: 72 step: 130 loss: 0.0003564258250408682\n",
            "epoch: 72 step: 140 loss: 0.0005586892856283639\n",
            "epoch: 72 step: 150 loss: 0.0003157847301488872\n",
            "epoch: 72 step: 160 loss: 0.0001296910882409567\n",
            "epoch: 72 step: 170 loss: 0.00032871180599700783\n",
            "epoch: 72 step: 180 loss: 0.00023377070410835773\n",
            "epoch: 72 finished! loss: 0.00023377070410835773 learning_rate: [0.0001]\n",
            "epoch: 73 step: 10 loss: 0.00028991791500570163\n",
            "epoch: 73 step: 20 loss: 0.0005999480631009609\n",
            "epoch: 73 step: 30 loss: 0.00024376729111582007\n",
            "epoch: 73 step: 40 loss: 0.0002583299721955348\n",
            "epoch: 73 step: 50 loss: 0.0003388489218032855\n",
            "epoch: 73 step: 60 loss: 0.000422637259750456\n",
            "epoch: 73 step: 70 loss: 0.0003346850830338166\n",
            "epoch: 73 step: 80 loss: 0.00012695684905132486\n",
            "epoch: 73 step: 90 loss: 0.000405631858741985\n",
            "epoch: 73 step: 100 loss: 0.0003480566542757871\n",
            "epoch: 73 step: 110 loss: 0.00026305052063837684\n",
            "epoch: 73 step: 120 loss: 0.00038367945688638294\n",
            "epoch: 73 step: 130 loss: 0.0003552004522416663\n",
            "epoch: 73 step: 140 loss: 0.0004171065584873065\n",
            "epoch: 73 step: 150 loss: 0.00015472048213084798\n",
            "epoch: 73 step: 160 loss: 0.0004789116549239682\n",
            "epoch: 73 step: 170 loss: 0.0002577745681200048\n",
            "epoch: 73 step: 180 loss: 0.00033851833032982274\n",
            "epoch: 73 finished! loss: 0.00033851833032982274 learning_rate: [0.0001]\n",
            "epoch: 74 step: 10 loss: 0.0002955310551563851\n",
            "epoch: 74 step: 20 loss: 0.00045808076976870026\n",
            "epoch: 74 step: 30 loss: 0.00022803661302710676\n",
            "epoch: 74 step: 40 loss: 0.00033376064696516796\n",
            "epoch: 74 step: 50 loss: 0.0005565759312055555\n",
            "epoch: 74 step: 60 loss: 0.00025873295584277577\n",
            "epoch: 74 step: 70 loss: 0.00025069008282826435\n",
            "epoch: 74 step: 80 loss: 0.0004825540709234877\n",
            "epoch: 74 step: 90 loss: 0.00041739461328033474\n",
            "epoch: 74 step: 100 loss: 0.00033935344168680646\n",
            "epoch: 74 step: 110 loss: 0.0004392854444890309\n",
            "epoch: 74 step: 120 loss: 0.00028942786488111316\n",
            "epoch: 74 step: 130 loss: 0.0003063552759356846\n",
            "epoch: 74 step: 140 loss: 0.00030434449109250456\n",
            "epoch: 74 step: 150 loss: 0.0005096184246693713\n",
            "epoch: 74 step: 160 loss: 0.00030065469232718763\n",
            "epoch: 74 step: 170 loss: 0.0002923875397566995\n",
            "epoch: 74 step: 180 loss: 0.00038778958738553635\n",
            "epoch: 74 finished! loss: 0.00038778958738553635 learning_rate: [0.0001]\n",
            "epoch: 75 step: 10 loss: 0.0002971160879339739\n",
            "epoch: 75 step: 20 loss: 0.0005486486531603078\n",
            "epoch: 75 step: 30 loss: 0.00022589300851119217\n",
            "epoch: 75 step: 40 loss: 0.0003437349140207046\n",
            "epoch: 75 step: 50 loss: 0.0006570232987026701\n",
            "epoch: 75 step: 60 loss: 0.00024060280988171377\n",
            "epoch: 75 step: 70 loss: 0.00032182580355600925\n",
            "epoch: 75 step: 80 loss: 0.0005318860971684527\n",
            "epoch: 75 step: 90 loss: 0.00014371988412264955\n",
            "epoch: 75 step: 100 loss: 0.00035084995123898664\n",
            "epoch: 75 step: 110 loss: 0.00022804519129687606\n",
            "epoch: 75 step: 120 loss: 0.0003089509301993744\n",
            "epoch: 75 step: 130 loss: 0.00023964474182328003\n",
            "epoch: 75 step: 140 loss: 0.0002799602423935572\n",
            "epoch: 75 step: 150 loss: 0.00023958623196097424\n",
            "epoch: 75 step: 160 loss: 0.00014527234202927722\n",
            "epoch: 75 step: 170 loss: 0.0003672374399579397\n",
            "epoch: 75 step: 180 loss: 0.00018685924106851006\n",
            "epoch: 75 finished! loss: 0.00018685924106851006 learning_rate: [0.0001]\n",
            "epoch: 76 step: 10 loss: 0.00029875859894154354\n",
            "epoch: 76 step: 20 loss: 0.00016483358207329765\n",
            "epoch: 76 step: 30 loss: 0.0006473750727290788\n",
            "epoch: 76 step: 40 loss: 0.00023261751788405184\n",
            "epoch: 76 step: 50 loss: 0.0003180803444327379\n",
            "epoch: 76 step: 60 loss: 0.00015706145075491704\n",
            "epoch: 76 step: 70 loss: 0.0005313393489434607\n",
            "epoch: 76 step: 80 loss: 0.000256414347725956\n",
            "epoch: 76 step: 90 loss: 0.00047585473477440967\n",
            "epoch: 76 step: 100 loss: 0.000316952942379963\n",
            "epoch: 76 step: 110 loss: 0.0006108813156954723\n",
            "epoch: 76 step: 120 loss: 0.00021866655173696572\n",
            "epoch: 76 step: 130 loss: 0.00033213703363172624\n",
            "epoch: 76 step: 140 loss: 0.0006281769525979719\n",
            "epoch: 76 step: 150 loss: 0.0003345249311353355\n",
            "epoch: 76 step: 160 loss: 0.0004861437968515691\n",
            "epoch: 76 step: 170 loss: 0.00026969657866601266\n",
            "epoch: 76 step: 180 loss: 0.0001920619652477608\n",
            "epoch: 76 finished! loss: 0.0001920619652477608 learning_rate: [0.0001]\n",
            "epoch: 77 step: 10 loss: 0.00040518973391139403\n",
            "epoch: 77 step: 20 loss: 0.0004658326029637948\n",
            "epoch: 77 step: 30 loss: 0.00017007062019725082\n",
            "epoch: 77 step: 40 loss: 0.00017216910257878478\n",
            "epoch: 77 step: 50 loss: 0.0003477767138383016\n",
            "epoch: 77 step: 60 loss: 0.0003618339319315957\n",
            "epoch: 77 step: 70 loss: 0.00021173107383960156\n",
            "epoch: 77 step: 80 loss: 0.0002971907915055434\n",
            "epoch: 77 step: 90 loss: 0.0003475962794165201\n",
            "epoch: 77 step: 100 loss: 0.00025773681814822126\n",
            "epoch: 77 step: 110 loss: 0.0002848803069078728\n",
            "epoch: 77 step: 120 loss: 0.00034725298166356856\n",
            "epoch: 77 step: 130 loss: 0.00024409312692288181\n",
            "epoch: 77 step: 140 loss: 0.00033513303506261155\n",
            "epoch: 77 step: 150 loss: 0.00040515301528481595\n",
            "epoch: 77 step: 160 loss: 0.00042702455284645565\n",
            "epoch: 77 step: 170 loss: 0.00028308830679309134\n",
            "epoch: 77 step: 180 loss: 0.0003390499259456241\n",
            "epoch: 77 finished! loss: 0.0003390499259456241 learning_rate: [0.0001]\n",
            "epoch: 78 step: 10 loss: 0.00013342642342115599\n",
            "epoch: 78 step: 20 loss: 0.00031917830541874437\n",
            "epoch: 78 step: 30 loss: 0.0002835981771721654\n",
            "epoch: 78 step: 40 loss: 0.0002092767184920861\n",
            "epoch: 78 step: 50 loss: 0.0003058576436140463\n",
            "epoch: 78 step: 60 loss: 0.00024143726789629133\n",
            "epoch: 78 step: 70 loss: 0.00014490815395139906\n",
            "epoch: 78 step: 80 loss: 0.00024074427382106666\n",
            "epoch: 78 step: 90 loss: 0.00024402808368140206\n",
            "epoch: 78 step: 100 loss: 0.0002600248089525823\n",
            "epoch: 78 step: 110 loss: 0.00027799731358035573\n",
            "epoch: 78 step: 120 loss: 0.00023411585826232898\n",
            "epoch: 78 step: 130 loss: 0.0003905785708614159\n",
            "epoch: 78 step: 140 loss: 0.00010176342053470193\n",
            "epoch: 78 step: 150 loss: 0.0003335708689471029\n",
            "epoch: 78 step: 160 loss: 0.0002454804805894507\n",
            "epoch: 78 step: 170 loss: 0.00039802667451178914\n",
            "epoch: 78 step: 180 loss: 0.00013458704502003192\n",
            "epoch: 78 finished! loss: 0.00013458704502003192 learning_rate: [0.0001]\n",
            "epoch: 79 step: 10 loss: 0.0002445464111459459\n",
            "epoch: 79 step: 20 loss: 0.00022353018348163884\n",
            "epoch: 79 step: 30 loss: 0.00026815676627017326\n",
            "epoch: 79 step: 40 loss: 0.000583508923605178\n",
            "epoch: 79 step: 50 loss: 0.0002510114898744497\n",
            "epoch: 79 step: 60 loss: 0.00025373062521526473\n",
            "epoch: 79 step: 70 loss: 0.0001698874931093938\n",
            "epoch: 79 step: 80 loss: 0.00022476652962206194\n",
            "epoch: 79 step: 90 loss: 0.0001868611409787449\n",
            "epoch: 79 step: 100 loss: 0.000331146541775849\n",
            "epoch: 79 step: 110 loss: 0.0001993939513213348\n",
            "epoch: 79 step: 120 loss: 0.00029322722051358777\n",
            "epoch: 79 step: 130 loss: 0.0002503356348542073\n",
            "epoch: 79 step: 140 loss: 0.00025691641411447827\n",
            "epoch: 79 step: 150 loss: 0.000180224046695217\n",
            "epoch: 79 step: 160 loss: 0.0003328980127653468\n",
            "epoch: 79 step: 170 loss: 0.00045024860747353804\n",
            "epoch: 79 step: 180 loss: 0.0002826787418857359\n",
            "epoch: 79 finished! loss: 0.0002826787418857359 learning_rate: [0.0001]\n",
            "epoch: 80 step: 10 loss: 0.00031018332476627497\n",
            "epoch: 80 step: 20 loss: 0.0003460445101825151\n",
            "epoch: 80 step: 30 loss: 0.0001567616184428089\n",
            "epoch: 80 step: 40 loss: 0.00020597080131275864\n",
            "epoch: 80 step: 50 loss: 0.0003556881997417598\n",
            "epoch: 80 step: 60 loss: 0.00046148535193318076\n",
            "epoch: 80 step: 70 loss: 0.000241602241385771\n",
            "epoch: 80 step: 80 loss: 0.0003732950696645717\n",
            "epoch: 80 step: 90 loss: 0.0001502259977378226\n",
            "epoch: 80 step: 100 loss: 0.0004136284837783068\n",
            "epoch: 80 step: 110 loss: 0.0002365906187640855\n",
            "epoch: 80 step: 120 loss: 0.0003206764897547288\n",
            "epoch: 80 step: 130 loss: 0.0003001081946190246\n",
            "epoch: 80 step: 140 loss: 0.00032620300375436564\n",
            "epoch: 80 step: 150 loss: 0.00014718279866503877\n",
            "epoch: 80 step: 160 loss: 0.00032829034868455634\n",
            "epoch: 80 step: 170 loss: 0.0003763452298277965\n",
            "epoch: 80 step: 180 loss: 0.00033699715274472086\n",
            "epoch: 80 finished! loss: 0.00033699715274472086 learning_rate: [0.0001]\n",
            "epoch: 81 step: 10 loss: 0.00017592206173059605\n",
            "epoch: 81 step: 20 loss: 0.00011288461143655788\n",
            "epoch: 81 step: 30 loss: 0.000353128532295688\n",
            "epoch: 81 step: 40 loss: 0.00015660092826086405\n",
            "epoch: 81 step: 50 loss: 0.0001924558668578927\n",
            "epoch: 81 step: 60 loss: 0.0003310647068215674\n",
            "epoch: 81 step: 70 loss: 0.00030269614082292605\n",
            "epoch: 81 step: 80 loss: 0.00036072966611442345\n",
            "epoch: 81 step: 90 loss: 0.00015713256907112476\n",
            "epoch: 81 step: 100 loss: 0.00032588014845618855\n",
            "epoch: 81 step: 110 loss: 0.00030291546488873976\n",
            "epoch: 81 step: 120 loss: 0.00018868731586225447\n",
            "epoch: 81 step: 130 loss: 0.00044999943901327395\n",
            "epoch: 81 step: 140 loss: 0.0002648395082734246\n",
            "epoch: 81 step: 150 loss: 0.0002551585725944126\n",
            "epoch: 81 step: 160 loss: 0.0001379610835438686\n",
            "epoch: 81 step: 170 loss: 0.0003282807759558047\n",
            "epoch: 81 step: 180 loss: 0.00035518178776718903\n",
            "epoch: 81 finished! loss: 0.00035518178776718903 learning_rate: [0.0001]\n",
            "epoch: 82 step: 10 loss: 0.0003703276136832501\n",
            "epoch: 82 step: 20 loss: 0.0002468726282644778\n",
            "epoch: 82 step: 30 loss: 0.00021913023496822996\n",
            "epoch: 82 step: 40 loss: 0.00020371624995080552\n",
            "epoch: 82 step: 50 loss: 0.00034834060100843095\n",
            "epoch: 82 step: 60 loss: 0.0007868323876536938\n",
            "epoch: 82 step: 70 loss: 0.0001488866063275402\n",
            "epoch: 82 step: 80 loss: 0.0003719266436028205\n",
            "epoch: 82 step: 90 loss: 0.00033868202308501186\n",
            "epoch: 82 step: 100 loss: 0.000511250347658507\n",
            "epoch: 82 step: 110 loss: 0.00037209987892258237\n",
            "epoch: 82 step: 120 loss: 0.0002862683943908205\n",
            "epoch: 82 step: 130 loss: 0.0001188847811883254\n",
            "epoch: 82 step: 140 loss: 0.0006381987093289798\n",
            "epoch: 82 step: 150 loss: 0.0002611240452370722\n",
            "epoch: 82 step: 160 loss: 0.000232299565112483\n",
            "epoch: 82 step: 170 loss: 0.00033314592491986045\n",
            "epoch: 82 step: 180 loss: 0.0002964319235713313\n",
            "epoch: 82 finished! loss: 0.0002964319235713313 learning_rate: [0.0001]\n",
            "epoch: 83 step: 10 loss: 0.00026842548949269\n",
            "epoch: 83 step: 20 loss: 0.0002788853315498307\n",
            "epoch: 83 step: 30 loss: 0.00047062007869706865\n",
            "epoch: 83 step: 40 loss: 0.000206197243933208\n",
            "epoch: 83 step: 50 loss: 0.0003036934341647697\n",
            "epoch: 83 step: 60 loss: 0.00020776245631170113\n",
            "epoch: 83 step: 70 loss: 0.00021353015965727782\n",
            "epoch: 83 step: 80 loss: 0.00015415753493530217\n",
            "epoch: 83 step: 90 loss: 0.000375869575744046\n",
            "epoch: 83 step: 100 loss: 0.00048695081879506484\n",
            "epoch: 83 step: 110 loss: 0.00036337856037909293\n",
            "epoch: 83 step: 120 loss: 0.00022264067671039643\n",
            "epoch: 83 step: 130 loss: 0.00024836115616234613\n",
            "epoch: 83 step: 140 loss: 0.00043675985691103654\n",
            "epoch: 83 step: 150 loss: 0.00039939407823657246\n",
            "epoch: 83 step: 160 loss: 0.00046987947108002257\n",
            "epoch: 83 step: 170 loss: 0.000421522424434072\n",
            "epoch: 83 step: 180 loss: 0.00021529430623173725\n",
            "epoch: 83 finished! loss: 0.00021529430623173725 learning_rate: [0.0001]\n",
            "epoch: 84 step: 10 loss: 0.00022996931489635173\n",
            "epoch: 84 step: 20 loss: 0.0001335705477809211\n",
            "epoch: 84 step: 30 loss: 0.00012141127681002562\n",
            "epoch: 84 step: 40 loss: 0.00044328973419959286\n",
            "epoch: 84 step: 50 loss: 0.00045018139088033186\n",
            "epoch: 84 step: 60 loss: 0.0003142464056546682\n",
            "epoch: 84 step: 70 loss: 0.00028108820594172464\n",
            "epoch: 84 step: 80 loss: 0.00018671611935635888\n",
            "epoch: 84 step: 90 loss: 0.0002589241968117705\n",
            "epoch: 84 step: 100 loss: 0.00047998324743882606\n",
            "epoch: 84 step: 110 loss: 0.00021593030784180129\n",
            "epoch: 84 step: 120 loss: 0.0002152371280597837\n",
            "epoch: 84 step: 130 loss: 0.0006231060490635895\n",
            "epoch: 84 step: 140 loss: 0.00019288370976402455\n",
            "epoch: 84 step: 150 loss: 0.00017166497450322483\n",
            "epoch: 84 step: 160 loss: 0.0002600965911561905\n",
            "epoch: 84 step: 170 loss: 0.0004207476756818695\n",
            "epoch: 84 step: 180 loss: 0.00028896765882872405\n",
            "epoch: 84 finished! loss: 0.00028896765882872405 learning_rate: [0.0001]\n",
            "epoch: 85 step: 10 loss: 0.0003168422422559519\n",
            "epoch: 85 step: 20 loss: 0.00021643370236785767\n",
            "epoch: 85 step: 30 loss: 0.00018661924071228415\n",
            "epoch: 85 step: 40 loss: 0.0005114364725838186\n",
            "epoch: 85 step: 50 loss: 0.00043128300792068236\n",
            "epoch: 85 step: 60 loss: 0.0001947648101951153\n",
            "epoch: 85 step: 70 loss: 0.00024703951026175495\n",
            "epoch: 85 step: 80 loss: 0.00019305143176566835\n",
            "epoch: 85 step: 90 loss: 9.3935992569665e-05\n",
            "epoch: 85 step: 100 loss: 0.00019873695652748035\n",
            "epoch: 85 step: 110 loss: 0.00028435580652987937\n",
            "epoch: 85 step: 120 loss: 0.00016790424478905488\n",
            "epoch: 85 step: 130 loss: 0.00017674032011427977\n",
            "epoch: 85 step: 140 loss: 0.00026357317967678074\n",
            "epoch: 85 step: 150 loss: 0.00024375228433418754\n",
            "epoch: 85 step: 160 loss: 0.0002223103749252347\n",
            "epoch: 85 step: 170 loss: 0.0004051766851053195\n",
            "epoch: 85 step: 180 loss: 0.00017122680326602993\n",
            "epoch: 85 finished! loss: 0.00017122680326602993 learning_rate: [0.0001]\n",
            "epoch: 86 step: 10 loss: 0.0004112189801958067\n",
            "epoch: 86 step: 20 loss: 0.00023523771878939146\n",
            "epoch: 86 step: 30 loss: 0.0003442508040365409\n",
            "epoch: 86 step: 40 loss: 0.0002619254484595428\n",
            "epoch: 86 step: 50 loss: 0.00015222545406865298\n",
            "epoch: 86 step: 60 loss: 0.0004286551798890603\n",
            "epoch: 86 step: 70 loss: 0.00025568931380793273\n",
            "epoch: 86 step: 80 loss: 0.0003423783137163437\n",
            "epoch: 86 step: 90 loss: 0.00045919945761121214\n",
            "epoch: 86 step: 100 loss: 0.0005209120695801061\n",
            "epoch: 86 step: 110 loss: 0.00019013543440673462\n",
            "epoch: 86 step: 120 loss: 0.00032284711458915286\n",
            "epoch: 86 step: 130 loss: 0.0002541614455562068\n",
            "epoch: 86 step: 140 loss: 0.0001817291301424177\n",
            "epoch: 86 step: 150 loss: 0.00031375906408643466\n",
            "epoch: 86 step: 160 loss: 0.00022009060159013674\n",
            "epoch: 86 step: 170 loss: 0.0005256229435617746\n",
            "epoch: 86 step: 180 loss: 0.00020008980739419762\n",
            "epoch: 86 finished! loss: 0.00020008980739419762 learning_rate: [0.0001]\n",
            "epoch: 87 step: 10 loss: 0.00023235226854017655\n",
            "epoch: 87 step: 20 loss: 0.000299913896915596\n",
            "epoch: 87 step: 30 loss: 0.00017722859459263671\n",
            "epoch: 87 step: 40 loss: 0.00016836309069654325\n",
            "epoch: 87 step: 50 loss: 0.0007127204676547939\n",
            "epoch: 87 step: 60 loss: 0.0007258790544514708\n",
            "epoch: 87 step: 70 loss: 0.0003792606109886719\n",
            "epoch: 87 step: 80 loss: 0.00031177361076908065\n",
            "epoch: 87 step: 90 loss: 0.001827210937224918\n",
            "epoch: 87 step: 100 loss: 0.000510353942214694\n",
            "epoch: 87 step: 110 loss: 0.00023046295047021763\n",
            "epoch: 87 step: 120 loss: 0.0003128239890112383\n",
            "epoch: 87 step: 130 loss: 0.0005003190558120323\n",
            "epoch: 87 step: 140 loss: 0.00037802789153040773\n",
            "epoch: 87 step: 150 loss: 0.000968555344447967\n",
            "epoch: 87 step: 160 loss: 0.000610893561370936\n",
            "epoch: 87 step: 170 loss: 0.0003395854327362031\n",
            "epoch: 87 step: 180 loss: 0.0002711878450105883\n",
            "epoch: 87 finished! loss: 0.0002711878450105883 learning_rate: [0.0001]\n",
            "epoch: 88 step: 10 loss: 0.00019931036513462603\n",
            "epoch: 88 step: 20 loss: 0.00018820843638584617\n",
            "epoch: 88 step: 30 loss: 0.00019393107215496817\n",
            "epoch: 88 step: 40 loss: 0.00027589473399410193\n",
            "epoch: 88 step: 50 loss: 0.0002982751605941621\n",
            "epoch: 88 step: 60 loss: 0.0005322018570015855\n",
            "epoch: 88 step: 70 loss: 0.00017939941391855713\n",
            "epoch: 88 step: 80 loss: 0.00011006627734341088\n",
            "epoch: 88 step: 90 loss: 0.000307697217848887\n",
            "epoch: 88 step: 100 loss: 0.0003658478933690491\n",
            "epoch: 88 step: 110 loss: 0.0006848601923319505\n",
            "epoch: 88 step: 120 loss: 0.00022176848315133554\n",
            "epoch: 88 step: 130 loss: 0.0007902004809987694\n",
            "epoch: 88 step: 140 loss: 0.00013153275738888814\n",
            "epoch: 88 step: 150 loss: 0.00025726013075215865\n",
            "epoch: 88 step: 160 loss: 0.0001889739421057788\n",
            "epoch: 88 step: 170 loss: 0.00016911151959903765\n",
            "epoch: 88 step: 180 loss: 0.00029172592377968974\n",
            "epoch: 88 finished! loss: 0.00029172592377968974 learning_rate: [0.0001]\n",
            "epoch: 89 step: 10 loss: 0.00016394914623356916\n",
            "epoch: 89 step: 20 loss: 0.00026290022755016815\n",
            "epoch: 89 step: 30 loss: 0.000223310960020571\n",
            "epoch: 89 step: 40 loss: 0.00017721951393814874\n",
            "epoch: 89 step: 50 loss: 0.00029172016636745023\n",
            "epoch: 89 step: 60 loss: 0.0002027533031339058\n",
            "epoch: 89 step: 70 loss: 0.0001335866109973509\n",
            "epoch: 89 step: 80 loss: 0.00014342784768387923\n",
            "epoch: 89 step: 90 loss: 0.0001740649272387486\n",
            "epoch: 89 step: 100 loss: 0.00023794910599752556\n",
            "epoch: 89 step: 110 loss: 0.00017269718194116589\n",
            "epoch: 89 step: 120 loss: 0.0001675067232566799\n",
            "epoch: 89 step: 130 loss: 0.0006612103954675946\n",
            "epoch: 89 step: 140 loss: 0.0005057267241839599\n",
            "epoch: 89 step: 150 loss: 0.0003591012996171579\n",
            "epoch: 89 step: 160 loss: 0.00022608275088796106\n",
            "epoch: 89 step: 170 loss: 0.0003190098233674708\n",
            "epoch: 89 step: 180 loss: 0.00023276032417571897\n",
            "epoch: 89 finished! loss: 0.00023276032417571897 learning_rate: [0.0001]\n",
            "epoch: 90 step: 10 loss: 0.00026727088329020547\n",
            "epoch: 90 step: 20 loss: 0.00030831755242197124\n",
            "epoch: 90 step: 30 loss: 0.00012120869213646482\n",
            "epoch: 90 step: 40 loss: 0.00042460132331638244\n",
            "epoch: 90 step: 50 loss: 0.0004694636333011043\n",
            "epoch: 90 step: 60 loss: 0.00019573579666136898\n",
            "epoch: 90 step: 70 loss: 0.00043487919478989585\n",
            "epoch: 90 step: 80 loss: 0.00023316518825495415\n",
            "epoch: 90 step: 90 loss: 0.0003028281478455439\n",
            "epoch: 90 step: 100 loss: 0.0006582793213760347\n",
            "epoch: 90 step: 110 loss: 0.00012569289177054044\n",
            "epoch: 90 step: 120 loss: 0.0002404095291394773\n",
            "epoch: 90 step: 130 loss: 0.0007029671926654536\n",
            "epoch: 90 step: 140 loss: 0.00027676554986979656\n",
            "epoch: 90 step: 150 loss: 0.0003784427731442479\n",
            "epoch: 90 step: 160 loss: 0.000220957488143932\n",
            "epoch: 90 step: 170 loss: 0.000101980470071199\n",
            "epoch: 90 step: 180 loss: 0.0002695289783651841\n",
            "epoch: 90 finished! loss: 0.0002695289783651841 learning_rate: [0.0001]\n",
            "epoch: 91 step: 10 loss: 0.0003985158403138375\n",
            "epoch: 91 step: 20 loss: 0.00017353271808735093\n",
            "epoch: 91 step: 30 loss: 0.00025562014865807844\n",
            "epoch: 91 step: 40 loss: 0.0004590689366692675\n",
            "epoch: 91 step: 50 loss: 0.00020587817523937932\n",
            "epoch: 91 step: 60 loss: 0.00016773177808816024\n",
            "epoch: 91 step: 70 loss: 0.00025682314050081047\n",
            "epoch: 91 step: 80 loss: 0.00019532765487843845\n",
            "epoch: 91 step: 90 loss: 0.00014824563328543812\n",
            "epoch: 91 step: 100 loss: 0.0003674343959586462\n",
            "epoch: 91 step: 110 loss: 0.0002180007446356358\n",
            "epoch: 91 step: 120 loss: 0.00021335546461917547\n",
            "epoch: 91 step: 130 loss: 0.00015911166366640463\n",
            "epoch: 91 step: 140 loss: 0.0002749934556109524\n",
            "epoch: 91 step: 150 loss: 0.00029053858497050624\n",
            "epoch: 91 step: 160 loss: 0.00023249501559393233\n",
            "epoch: 91 step: 170 loss: 0.00010435222928746549\n",
            "epoch: 91 step: 180 loss: 0.00036414786674496567\n",
            "epoch: 91 finished! loss: 0.00036414786674496567 learning_rate: [2.5e-05]\n",
            "epoch: 92 step: 10 loss: 0.0002316757508104483\n",
            "epoch: 92 step: 20 loss: 0.0001314973870720153\n",
            "epoch: 92 step: 30 loss: 0.00022580487138988274\n",
            "epoch: 92 step: 40 loss: 0.0002760357302157068\n",
            "epoch: 92 step: 50 loss: 0.0003533375834430685\n",
            "epoch: 92 step: 60 loss: 0.00029573112471967456\n",
            "epoch: 92 step: 70 loss: 0.00023590378579639928\n",
            "epoch: 92 step: 80 loss: 0.00024152791500823762\n",
            "epoch: 92 step: 90 loss: 0.0004902643042036519\n",
            "epoch: 92 step: 100 loss: 0.00030164500976651563\n",
            "epoch: 92 step: 110 loss: 0.00021837359351856297\n",
            "epoch: 92 step: 120 loss: 0.00017312754855739148\n",
            "epoch: 92 step: 130 loss: 0.0002414833525335904\n",
            "epoch: 92 step: 140 loss: 0.00032730006317960915\n",
            "epoch: 92 step: 150 loss: 0.0003792671187513276\n",
            "epoch: 92 step: 160 loss: 0.000370083726083409\n",
            "epoch: 92 step: 170 loss: 0.00020086456835597568\n",
            "epoch: 92 step: 180 loss: 0.00013551367427036502\n",
            "epoch: 92 finished! loss: 0.00013551367427036502 learning_rate: [5e-05]\n",
            "epoch: 93 step: 10 loss: 0.0001410304064917677\n",
            "epoch: 93 step: 20 loss: 0.00030746345555539935\n",
            "epoch: 93 step: 30 loss: 0.00040343824963197264\n",
            "epoch: 93 step: 40 loss: 0.00015783283378905066\n",
            "epoch: 93 step: 50 loss: 0.0004933856701741555\n",
            "epoch: 93 step: 60 loss: 0.00018202331461181115\n",
            "epoch: 93 step: 70 loss: 0.00017390962178067337\n",
            "epoch: 93 step: 80 loss: 7.325540346055899e-05\n",
            "epoch: 93 step: 90 loss: 0.0001061653116898562\n",
            "epoch: 93 step: 100 loss: 0.00014530115144644577\n",
            "epoch: 93 step: 110 loss: 0.0001700341038457371\n",
            "epoch: 93 step: 120 loss: 0.000179262216511092\n",
            "epoch: 93 step: 130 loss: 0.00027165250883248043\n",
            "epoch: 93 step: 140 loss: 0.0001635994194039529\n",
            "epoch: 93 step: 150 loss: 0.0003511484753018195\n",
            "epoch: 93 step: 160 loss: 0.0002695134325563346\n",
            "epoch: 93 step: 170 loss: 9.28130992469845e-05\n",
            "epoch: 93 step: 180 loss: 0.00018074047695446508\n",
            "epoch: 93 finished! loss: 0.00018074047695446508 learning_rate: [5e-05]\n",
            "epoch: 94 step: 10 loss: 0.00022445122694101898\n",
            "epoch: 94 step: 20 loss: 0.0001877436571396313\n",
            "epoch: 94 step: 30 loss: 0.000337997241797983\n",
            "epoch: 94 step: 40 loss: 0.00012186253635396656\n",
            "epoch: 94 step: 50 loss: 0.000334123408403228\n",
            "epoch: 94 step: 60 loss: 0.000119385460824015\n",
            "epoch: 94 step: 70 loss: 0.00032370753950135755\n",
            "epoch: 94 step: 80 loss: 0.00020559318926870366\n",
            "epoch: 94 step: 90 loss: 0.00012441151195277306\n",
            "epoch: 94 step: 100 loss: 0.00015649259470721898\n",
            "epoch: 94 step: 110 loss: 0.00018130609297477483\n",
            "epoch: 94 step: 120 loss: 0.0002797303911256552\n",
            "epoch: 94 step: 130 loss: 0.0001343156286180242\n",
            "epoch: 94 step: 140 loss: 0.00027054981809892267\n",
            "epoch: 94 step: 150 loss: 0.00016061127050502488\n",
            "epoch: 94 step: 160 loss: 0.00023093185227952124\n",
            "epoch: 94 step: 170 loss: 0.00025023452162807396\n",
            "epoch: 94 step: 180 loss: 0.000285885604388747\n",
            "epoch: 94 finished! loss: 0.000285885604388747 learning_rate: [5e-05]\n",
            "epoch: 95 step: 10 loss: 0.000356988411478287\n",
            "epoch: 95 step: 20 loss: 0.00015927028641080944\n",
            "epoch: 95 step: 30 loss: 0.00010640345377498469\n",
            "epoch: 95 step: 40 loss: 0.0005396219007812726\n",
            "epoch: 95 step: 50 loss: 0.00033091163464845724\n",
            "epoch: 95 step: 60 loss: 0.00033764915637225583\n",
            "epoch: 95 step: 70 loss: 0.000438878046163945\n",
            "epoch: 95 step: 80 loss: 8.200932787766845e-05\n",
            "epoch: 95 step: 90 loss: 0.0002574686679816734\n",
            "epoch: 95 step: 100 loss: 0.00017704592486656237\n",
            "epoch: 95 step: 110 loss: 0.00012712067165275182\n",
            "epoch: 95 step: 120 loss: 0.0002867231283383535\n",
            "epoch: 95 step: 130 loss: 0.00012937139213160974\n",
            "epoch: 95 step: 140 loss: 0.00019761789315530119\n",
            "epoch: 95 step: 150 loss: 0.00046046100741592086\n",
            "epoch: 95 step: 160 loss: 0.0002670466144379651\n",
            "epoch: 95 step: 170 loss: 0.0003013483824980837\n",
            "epoch: 95 step: 180 loss: 0.0002259279207349669\n",
            "epoch: 95 finished! loss: 0.0002259279207349669 learning_rate: [5e-05]\n",
            "epoch: 96 step: 10 loss: 0.00014315327184886024\n",
            "epoch: 96 step: 20 loss: 0.00013532770739685653\n",
            "epoch: 96 step: 30 loss: 0.00016143704213003197\n",
            "epoch: 96 step: 40 loss: 0.00038381496941151776\n",
            "epoch: 96 step: 50 loss: 9.955805380071693e-05\n",
            "epoch: 96 step: 60 loss: 0.0003683258622024749\n",
            "epoch: 96 step: 70 loss: 0.00043575789525518725\n",
            "epoch: 96 step: 80 loss: 0.00031832996194153403\n",
            "epoch: 96 step: 90 loss: 0.00036987487714902053\n",
            "epoch: 96 step: 100 loss: 0.00035189334676902673\n",
            "epoch: 96 step: 110 loss: 0.00015551763821847268\n",
            "epoch: 96 step: 120 loss: 0.0001312590783281908\n",
            "epoch: 96 step: 130 loss: 0.00022745553970823359\n",
            "epoch: 96 step: 140 loss: 0.0002130967358526454\n",
            "epoch: 96 step: 150 loss: 0.0002062087389671103\n",
            "epoch: 96 step: 160 loss: 0.0002262104718881683\n",
            "epoch: 96 step: 170 loss: 0.0005300644482973644\n",
            "epoch: 96 step: 180 loss: 0.00032767006129382316\n",
            "epoch: 96 finished! loss: 0.00032767006129382316 learning_rate: [5e-05]\n",
            "epoch: 97 step: 10 loss: 9.977821593471906e-05\n",
            "epoch: 97 step: 20 loss: 7.559257222397349e-05\n",
            "epoch: 97 step: 30 loss: 0.0001779552585889289\n",
            "epoch: 97 step: 40 loss: 0.00013596298167166098\n",
            "epoch: 97 step: 50 loss: 0.0001431452881970042\n",
            "epoch: 97 step: 60 loss: 0.00014400215255456775\n",
            "epoch: 97 step: 70 loss: 0.0003908135452739479\n",
            "epoch: 97 step: 80 loss: 0.00021592825773968736\n",
            "epoch: 97 step: 90 loss: 0.00023592428098526068\n",
            "epoch: 97 step: 100 loss: 0.0002733991015630638\n",
            "epoch: 97 step: 110 loss: 0.0002549794628510461\n",
            "epoch: 97 step: 120 loss: 0.0002856485207904936\n",
            "epoch: 97 step: 130 loss: 0.0002087355815487191\n",
            "epoch: 97 step: 140 loss: 0.00044693429871271923\n",
            "epoch: 97 step: 150 loss: 0.00017916658074479676\n",
            "epoch: 97 step: 160 loss: 0.0002439813117100933\n",
            "epoch: 97 step: 170 loss: 0.0001619967230520465\n",
            "epoch: 97 step: 180 loss: 0.00016825745850573688\n",
            "epoch: 97 finished! loss: 0.00016825745850573688 learning_rate: [5e-05]\n",
            "epoch: 98 step: 10 loss: 0.00018070881740860377\n",
            "epoch: 98 step: 20 loss: 0.00017530864804146895\n",
            "epoch: 98 step: 30 loss: 0.00034560448478412875\n",
            "epoch: 98 step: 40 loss: 0.0003641692313504021\n",
            "epoch: 98 step: 50 loss: 0.00010985571930752358\n",
            "epoch: 98 step: 60 loss: 0.0002518471548225397\n",
            "epoch: 98 step: 70 loss: 0.0003484979867561166\n",
            "epoch: 98 step: 80 loss: 0.00013630488959590173\n",
            "epoch: 98 step: 90 loss: 0.00012852052799332327\n",
            "epoch: 98 step: 100 loss: 0.0002997980271454479\n",
            "epoch: 98 step: 110 loss: 0.00013436241259423445\n",
            "epoch: 98 step: 120 loss: 0.00011511320831292657\n",
            "epoch: 98 step: 130 loss: 0.00018058834511558392\n",
            "epoch: 98 step: 140 loss: 0.0002633852982798626\n",
            "epoch: 98 step: 150 loss: 0.00023768417814446527\n",
            "epoch: 98 step: 160 loss: 0.00018919570867312178\n",
            "epoch: 98 step: 170 loss: 0.0001939904998020914\n",
            "epoch: 98 step: 180 loss: 0.00017202534531564147\n",
            "epoch: 98 finished! loss: 0.00017202534531564147 learning_rate: [5e-05]\n",
            "epoch: 99 step: 10 loss: 0.0001506364134835285\n",
            "epoch: 99 step: 20 loss: 0.0002960198849054608\n",
            "epoch: 99 step: 30 loss: 0.00015044816051735526\n",
            "epoch: 99 step: 40 loss: 0.0002544545350436143\n",
            "epoch: 99 step: 50 loss: 0.00028500077064703885\n",
            "epoch: 99 step: 60 loss: 0.00013670215433859618\n",
            "epoch: 99 step: 70 loss: 0.00016839971273827848\n",
            "epoch: 99 step: 80 loss: 0.00010871987702793167\n",
            "epoch: 99 step: 90 loss: 0.00019743258524063388\n",
            "epoch: 99 step: 100 loss: 0.00017185956497119874\n",
            "epoch: 99 step: 110 loss: 0.00028387118755765815\n",
            "epoch: 99 step: 120 loss: 0.0007292069891041495\n",
            "epoch: 99 step: 130 loss: 0.00034522952433457994\n",
            "epoch: 99 step: 140 loss: 0.0002816058412898945\n",
            "epoch: 99 step: 150 loss: 0.0002696578131533642\n",
            "epoch: 99 step: 160 loss: 0.00022141622231395232\n",
            "epoch: 99 step: 170 loss: 9.314499194298805e-05\n",
            "epoch: 99 step: 180 loss: 0.00023635830293354876\n",
            "epoch: 99 finished! loss: 0.00023635830293354876 learning_rate: [5e-05]\n",
            "epoch: 100 step: 10 loss: 0.00010456118659411614\n",
            "epoch: 100 step: 20 loss: 0.0002166066680011634\n",
            "epoch: 100 step: 30 loss: 0.0001935913824613179\n",
            "epoch: 100 step: 40 loss: 0.0002757266703450122\n",
            "epoch: 100 step: 50 loss: 0.00013112125440360609\n",
            "epoch: 100 step: 60 loss: 0.0001369501643885115\n",
            "epoch: 100 step: 70 loss: 0.00033038681432810354\n",
            "epoch: 100 step: 80 loss: 0.00011046331759174373\n",
            "epoch: 100 step: 90 loss: 0.00020239356783015576\n",
            "epoch: 100 step: 100 loss: 0.0002619640917041024\n",
            "epoch: 100 step: 110 loss: 0.00021529495233887992\n",
            "epoch: 100 step: 120 loss: 0.00024196341975354614\n",
            "epoch: 100 step: 130 loss: 0.00017850931160538365\n",
            "epoch: 100 step: 140 loss: 0.00026263665780077274\n",
            "epoch: 100 step: 150 loss: 0.00018920751298725214\n",
            "epoch: 100 step: 160 loss: 0.00014976524953106882\n",
            "epoch: 100 step: 170 loss: 0.000321456825627681\n",
            "epoch: 100 step: 180 loss: 0.00019215503788169535\n",
            "epoch: 100 finished! loss: 0.00019215503788169535 learning_rate: [5e-05]\n",
            "epoch: 101 step: 10 loss: 0.0002881659314809128\n",
            "epoch: 101 step: 20 loss: 0.0001351339715981884\n",
            "epoch: 101 step: 30 loss: 0.0003224424488362381\n",
            "epoch: 101 step: 40 loss: 0.00010302121996783677\n",
            "epoch: 101 step: 50 loss: 0.00011603576510067135\n",
            "epoch: 101 step: 60 loss: 0.0001937484050838854\n",
            "epoch: 101 step: 70 loss: 0.00023136191848795886\n",
            "epoch: 101 step: 80 loss: 0.00027421491775620675\n",
            "epoch: 101 step: 90 loss: 0.00010723756620986976\n",
            "epoch: 101 step: 100 loss: 0.0001622764565110612\n",
            "epoch: 101 step: 110 loss: 0.0005179257437090052\n",
            "epoch: 101 step: 120 loss: 0.00017694641354071403\n",
            "epoch: 101 step: 130 loss: 0.00010185546656688259\n",
            "epoch: 101 step: 140 loss: 0.0002929859126757507\n",
            "epoch: 101 step: 150 loss: 0.0002636325288846962\n",
            "epoch: 101 step: 160 loss: 6.88504849050904e-05\n",
            "epoch: 101 step: 170 loss: 7.341707174617721e-05\n",
            "epoch: 101 step: 180 loss: 0.00013567876977546052\n",
            "epoch: 101 finished! loss: 0.00013567876977546052 learning_rate: [5e-05]\n",
            "epoch: 102 step: 10 loss: 0.00020893454541228522\n",
            "epoch: 102 step: 20 loss: 0.0001997152819076867\n",
            "epoch: 102 step: 30 loss: 0.00014540616840962042\n",
            "epoch: 102 step: 40 loss: 0.00030548123534706966\n",
            "epoch: 102 step: 50 loss: 0.0005285862607834626\n",
            "epoch: 102 step: 60 loss: 0.00019251676586863186\n",
            "epoch: 102 step: 70 loss: 0.00015956543386956996\n",
            "epoch: 102 step: 80 loss: 0.00018342875242618662\n",
            "epoch: 102 step: 90 loss: 0.000256425446620637\n",
            "epoch: 102 step: 100 loss: 0.00014210790585322613\n",
            "epoch: 102 step: 110 loss: 0.00018658948954588286\n",
            "epoch: 102 step: 120 loss: 0.0001556717225341118\n",
            "epoch: 102 step: 130 loss: 0.00020039933405390013\n",
            "epoch: 102 step: 140 loss: 0.00020443497506442487\n",
            "epoch: 102 step: 150 loss: 0.00012881754569156424\n",
            "epoch: 102 step: 160 loss: 0.00011859371881576943\n",
            "epoch: 102 step: 170 loss: 0.00022734169603548225\n",
            "epoch: 102 step: 180 loss: 6.436956206942856e-05\n",
            "epoch: 102 finished! loss: 6.436956206942856e-05 learning_rate: [5e-05]\n",
            "epoch: 103 step: 10 loss: 0.00020019895082925074\n",
            "epoch: 103 step: 20 loss: 0.00017455500692596798\n",
            "epoch: 103 step: 30 loss: 0.00022728902109104884\n",
            "epoch: 103 step: 40 loss: 6.730887754897602e-05\n",
            "epoch: 103 step: 50 loss: 0.00034036670364638545\n",
            "epoch: 103 step: 60 loss: 0.00021056529711035237\n",
            "epoch: 103 step: 70 loss: 0.00048270365871810997\n",
            "epoch: 103 step: 80 loss: 0.00014382778388496312\n",
            "epoch: 103 step: 90 loss: 0.00018613903285752016\n",
            "epoch: 103 step: 100 loss: 0.00028325978922369107\n",
            "epoch: 103 step: 110 loss: 0.00022542243002204865\n",
            "epoch: 103 step: 120 loss: 0.0004495206245901605\n",
            "epoch: 103 step: 130 loss: 0.00016383678988044354\n",
            "epoch: 103 step: 140 loss: 0.00013795219422314264\n",
            "epoch: 103 step: 150 loss: 0.0002858661082743227\n",
            "epoch: 103 step: 160 loss: 0.00017895536838307612\n",
            "epoch: 103 step: 170 loss: 0.00025594169872568206\n",
            "epoch: 103 step: 180 loss: 0.00013369726351927018\n",
            "epoch: 103 finished! loss: 0.00013369726351927018 learning_rate: [5e-05]\n",
            "epoch: 104 step: 10 loss: 0.00012524501235330025\n",
            "epoch: 104 step: 20 loss: 0.00023449818517643888\n",
            "epoch: 104 step: 30 loss: 0.0002244167371416655\n",
            "epoch: 104 step: 40 loss: 0.00021001633549097015\n",
            "epoch: 104 step: 50 loss: 0.0002617914066962318\n",
            "epoch: 104 step: 60 loss: 0.0002455466646936959\n",
            "epoch: 104 step: 70 loss: 0.00011956597032117075\n",
            "epoch: 104 step: 80 loss: 0.00010388817226383848\n",
            "epoch: 104 step: 90 loss: 0.00010474827830220717\n",
            "epoch: 104 step: 100 loss: 0.00013886297019975136\n",
            "epoch: 104 step: 110 loss: 0.00023370185083304836\n",
            "epoch: 104 step: 120 loss: 0.00024905399346766057\n",
            "epoch: 104 step: 130 loss: 0.00014301482821156642\n",
            "epoch: 104 step: 140 loss: 0.0002922043075125909\n",
            "epoch: 104 step: 150 loss: 0.0005300614668665341\n",
            "epoch: 104 step: 160 loss: 0.00013630690274346374\n",
            "epoch: 104 step: 170 loss: 0.00044663632085076993\n",
            "epoch: 104 step: 180 loss: 0.00022783111970279947\n",
            "epoch: 104 finished! loss: 0.00022783111970279947 learning_rate: [5e-05]\n",
            "epoch: 105 step: 10 loss: 0.0001902904354149824\n",
            "epoch: 105 step: 20 loss: 0.00011560420365303581\n",
            "epoch: 105 step: 30 loss: 0.0002045917298006364\n",
            "epoch: 105 step: 40 loss: 0.00010456055425344293\n",
            "epoch: 105 step: 50 loss: 0.00018783703741895002\n",
            "epoch: 105 step: 60 loss: 0.0003732346215902937\n",
            "epoch: 105 step: 70 loss: 0.00025580416133331345\n",
            "epoch: 105 step: 80 loss: 0.0002612215603837347\n",
            "epoch: 105 step: 90 loss: 0.00013157919782760507\n",
            "epoch: 105 step: 100 loss: 0.00011916376991595274\n",
            "epoch: 105 step: 110 loss: 0.00021475663229617435\n",
            "epoch: 105 step: 120 loss: 0.00016419536045410256\n",
            "epoch: 105 step: 130 loss: 8.622945022551098e-05\n",
            "epoch: 105 step: 140 loss: 0.00016459882697560826\n",
            "epoch: 105 step: 150 loss: 0.00013579396449096555\n",
            "epoch: 105 step: 160 loss: 0.00017971835009850793\n",
            "epoch: 105 step: 170 loss: 0.00026747892417751043\n",
            "epoch: 105 step: 180 loss: 0.0003811871940964378\n",
            "epoch: 105 finished! loss: 0.0003811871940964378 learning_rate: [5e-05]\n",
            "epoch: 106 step: 10 loss: 0.00015178480371530624\n",
            "epoch: 106 step: 20 loss: 0.00018413883818484093\n",
            "epoch: 106 step: 30 loss: 0.00044755241356756977\n",
            "epoch: 106 step: 40 loss: 0.0002593143799076193\n",
            "epoch: 106 step: 50 loss: 0.0003457560323471379\n",
            "epoch: 106 step: 60 loss: 0.00019866829095560552\n",
            "epoch: 106 step: 70 loss: 0.0002948557692203922\n",
            "epoch: 106 step: 80 loss: 0.0002116827850041644\n",
            "epoch: 106 step: 90 loss: 0.00026694107628065506\n",
            "epoch: 106 step: 100 loss: 0.0001717270766831699\n",
            "epoch: 106 step: 110 loss: 0.00027572111429718954\n",
            "epoch: 106 step: 120 loss: 0.00012673995798933948\n",
            "epoch: 106 step: 130 loss: 0.00013855302709596864\n",
            "epoch: 106 step: 140 loss: 7.431698366911253e-05\n",
            "epoch: 106 step: 150 loss: 0.00025669372970498047\n",
            "epoch: 106 step: 160 loss: 0.00034200058586847176\n",
            "epoch: 106 step: 170 loss: 0.00031628892382852205\n",
            "epoch: 106 step: 180 loss: 0.0001666460778727844\n",
            "epoch: 106 finished! loss: 0.0001666460778727844 learning_rate: [5e-05]\n",
            "epoch: 107 step: 10 loss: 0.0001596749378214661\n",
            "epoch: 107 step: 20 loss: 0.00025192289869991745\n",
            "epoch: 107 step: 30 loss: 0.00022011529859937597\n",
            "epoch: 107 step: 40 loss: 0.00026151742168866945\n",
            "epoch: 107 step: 50 loss: 0.00012078949007458691\n",
            "epoch: 107 step: 60 loss: 0.00023915955812296748\n",
            "epoch: 107 step: 70 loss: 0.0001386936013222992\n",
            "epoch: 107 step: 80 loss: 0.00016596971580297605\n",
            "epoch: 107 step: 90 loss: 0.00011926422521250529\n",
            "epoch: 107 step: 100 loss: 0.0002957708386810046\n",
            "epoch: 107 step: 110 loss: 7.931959017935133e-05\n",
            "epoch: 107 step: 120 loss: 0.00018660711397241499\n",
            "epoch: 107 step: 130 loss: 0.00014251881069640048\n",
            "epoch: 107 step: 140 loss: 0.00042245963885359256\n",
            "epoch: 107 step: 150 loss: 0.00012298485330473056\n",
            "epoch: 107 step: 160 loss: 0.00031007047162479864\n",
            "epoch: 107 step: 170 loss: 9.114932838466118e-05\n",
            "epoch: 107 step: 180 loss: 0.00022611364317918136\n",
            "epoch: 107 finished! loss: 0.00022611364317918136 learning_rate: [5e-05]\n",
            "epoch: 108 step: 10 loss: 0.00017822472051880135\n",
            "epoch: 108 step: 20 loss: 0.00023769362775611435\n",
            "epoch: 108 step: 30 loss: 0.00011132718419416\n",
            "epoch: 108 step: 40 loss: 0.00013784574489568424\n",
            "epoch: 108 step: 50 loss: 0.00021667027948289863\n",
            "epoch: 108 step: 60 loss: 0.0002222656369463251\n",
            "epoch: 108 step: 70 loss: 0.0002742116781734844\n",
            "epoch: 108 step: 80 loss: 0.0002954899354827405\n",
            "epoch: 108 step: 90 loss: 0.00013974458084017248\n",
            "epoch: 108 step: 100 loss: 8.05943780289873e-05\n",
            "epoch: 108 step: 110 loss: 0.000257172207736884\n",
            "epoch: 108 step: 120 loss: 6.6684714125777e-05\n",
            "epoch: 108 step: 130 loss: 0.0001804381562812917\n",
            "epoch: 108 step: 140 loss: 0.00015649493757301458\n",
            "epoch: 108 step: 150 loss: 0.00011823437516042611\n",
            "epoch: 108 step: 160 loss: 0.0001735113216406434\n",
            "epoch: 108 step: 170 loss: 0.00042968607252875924\n",
            "epoch: 108 step: 180 loss: 0.0003260882559509315\n",
            "epoch: 108 finished! loss: 0.0003260882559509315 learning_rate: [5e-05]\n",
            "epoch: 109 step: 10 loss: 0.00015545218181125137\n",
            "epoch: 109 step: 20 loss: 0.0002956320663822458\n",
            "epoch: 109 step: 30 loss: 0.00016630814636332095\n",
            "epoch: 109 step: 40 loss: 0.0001531546527870253\n",
            "epoch: 109 step: 50 loss: 0.00016706254943465356\n",
            "epoch: 109 step: 60 loss: 0.000389327417155746\n",
            "epoch: 109 step: 70 loss: 9.541187552935328e-05\n",
            "epoch: 109 step: 80 loss: 0.00011434671445892936\n",
            "epoch: 109 step: 90 loss: 0.00013290125810088284\n",
            "epoch: 109 step: 100 loss: 0.00015407547521647624\n",
            "epoch: 109 step: 110 loss: 0.000115447460345622\n",
            "epoch: 109 step: 120 loss: 0.0002297597749051566\n",
            "epoch: 109 step: 130 loss: 0.00010293748619238718\n",
            "epoch: 109 step: 140 loss: 0.00022558588043839465\n",
            "epoch: 109 step: 150 loss: 8.274606576305365e-05\n",
            "epoch: 109 step: 160 loss: 0.0001309504712196533\n",
            "epoch: 109 step: 170 loss: 0.0001650506179220198\n",
            "epoch: 109 step: 180 loss: 0.0001808239469484751\n",
            "epoch: 109 finished! loss: 0.0001808239469484751 learning_rate: [5e-05]\n",
            "epoch: 110 step: 10 loss: 0.00021948015520561453\n",
            "epoch: 110 step: 20 loss: 0.00016276789678765623\n",
            "epoch: 110 step: 30 loss: 0.0005035807993684383\n",
            "epoch: 110 step: 40 loss: 0.0004798865782087185\n",
            "epoch: 110 step: 50 loss: 0.0001737880414228544\n",
            "epoch: 110 step: 60 loss: 0.00021103693342455448\n",
            "epoch: 110 step: 70 loss: 0.00038836441767027855\n",
            "epoch: 110 step: 80 loss: 0.0004800183150967277\n",
            "epoch: 110 step: 90 loss: 0.00047679970964243664\n",
            "epoch: 110 step: 100 loss: 0.00025880275985531623\n",
            "epoch: 110 step: 110 loss: 0.0002535269495393013\n",
            "epoch: 110 step: 120 loss: 0.00013535586946102307\n",
            "epoch: 110 step: 130 loss: 0.0003659998786233068\n",
            "epoch: 110 step: 140 loss: 0.00016320206703976584\n",
            "epoch: 110 step: 150 loss: 0.00020153722742747342\n",
            "epoch: 110 step: 160 loss: 0.0002366044733769332\n",
            "epoch: 110 step: 170 loss: 0.00033455105630965864\n",
            "epoch: 110 step: 180 loss: 0.00039254194592985375\n",
            "epoch: 110 finished! loss: 0.00039254194592985375 learning_rate: [5e-05]\n",
            "epoch: 111 step: 10 loss: 5.895481179886693e-05\n",
            "epoch: 111 step: 20 loss: 0.00013432854928879809\n",
            "epoch: 111 step: 30 loss: 0.0002832814455535384\n",
            "epoch: 111 step: 40 loss: 0.0004444391545514773\n",
            "epoch: 111 step: 50 loss: 0.0001305465651655819\n",
            "epoch: 111 step: 60 loss: 0.0002567341537268759\n",
            "epoch: 111 step: 70 loss: 0.00026630143285102253\n",
            "epoch: 111 step: 80 loss: 0.00010958503085931725\n",
            "epoch: 111 step: 90 loss: 0.0001351502123585682\n",
            "epoch: 111 step: 100 loss: 0.00012827992425504766\n",
            "epoch: 111 step: 110 loss: 0.000167608043139346\n",
            "epoch: 111 step: 120 loss: 0.0001410033055862882\n",
            "epoch: 111 step: 130 loss: 0.00010267432947607263\n",
            "epoch: 111 step: 140 loss: 0.00020934860935212512\n",
            "epoch: 111 step: 150 loss: 0.00024957831507939603\n",
            "epoch: 111 step: 160 loss: 0.00015218782615089173\n",
            "epoch: 111 step: 170 loss: 0.00023654104396065052\n",
            "epoch: 111 step: 180 loss: 0.00026189861310161515\n",
            "epoch: 111 finished! loss: 0.00026189861310161515 learning_rate: [1.25e-05]\n",
            "epoch: 112 step: 10 loss: 0.0002756129065494547\n",
            "epoch: 112 step: 20 loss: 0.00014019135255580523\n",
            "epoch: 112 step: 30 loss: 0.0001807435353627379\n",
            "epoch: 112 step: 40 loss: 0.0001453746176799455\n",
            "epoch: 112 step: 50 loss: 0.0001805181533268771\n",
            "epoch: 112 step: 60 loss: 0.0001649250994789595\n",
            "epoch: 112 step: 70 loss: 0.00019762587262966283\n",
            "epoch: 112 step: 80 loss: 0.0002004796659308957\n",
            "epoch: 112 step: 90 loss: 0.00026842102500350775\n",
            "epoch: 112 step: 100 loss: 0.00011134240044618811\n",
            "epoch: 112 step: 110 loss: 0.0001425782416176364\n",
            "epoch: 112 step: 120 loss: 0.0002165215350471006\n",
            "epoch: 112 step: 130 loss: 0.00013094946864742425\n",
            "epoch: 112 step: 140 loss: 0.00013475993685885422\n",
            "epoch: 112 step: 150 loss: 0.0001752186357828737\n",
            "epoch: 112 step: 160 loss: 0.00022945837905937662\n",
            "epoch: 112 step: 170 loss: 0.00019639987291002162\n",
            "epoch: 112 step: 180 loss: 0.00021449591652194816\n",
            "epoch: 112 finished! loss: 0.00021449591652194816 learning_rate: [2.5e-05]\n",
            "epoch: 113 step: 10 loss: 8.749639536951845e-05\n",
            "epoch: 113 step: 20 loss: 0.00018388445797009952\n",
            "epoch: 113 step: 30 loss: 0.00010577739012541894\n",
            "epoch: 113 step: 40 loss: 0.00014891440564914938\n",
            "epoch: 113 step: 50 loss: 0.00014219049333376709\n",
            "epoch: 113 step: 60 loss: 0.00022142949238126107\n",
            "epoch: 113 step: 70 loss: 0.00014761428250510607\n",
            "epoch: 113 step: 80 loss: 0.0002756188097849034\n",
            "epoch: 113 step: 90 loss: 0.00025473295034006866\n",
            "epoch: 113 step: 100 loss: 0.0003045995569632821\n",
            "epoch: 113 step: 110 loss: 0.00012247713293300668\n",
            "epoch: 113 step: 120 loss: 0.00018637920063446163\n",
            "epoch: 113 step: 130 loss: 0.0002874421356971753\n",
            "epoch: 113 step: 140 loss: 0.0004222496148289964\n",
            "epoch: 113 step: 150 loss: 0.00022082203238667127\n",
            "epoch: 113 step: 160 loss: 0.0003016499732910638\n",
            "epoch: 113 step: 170 loss: 0.00013981130395582947\n",
            "epoch: 113 step: 180 loss: 0.0002710876859394151\n",
            "epoch: 113 finished! loss: 0.0002710876859394151 learning_rate: [2.5e-05]\n",
            "epoch: 114 step: 10 loss: 0.00011573959050801146\n",
            "epoch: 114 step: 20 loss: 9.669628695179563e-05\n",
            "epoch: 114 step: 30 loss: 0.00012411732864171636\n",
            "epoch: 114 step: 40 loss: 0.00022144031082106468\n",
            "epoch: 114 step: 50 loss: 0.00029576246946314553\n",
            "epoch: 114 step: 60 loss: 0.00012300342343837508\n",
            "epoch: 114 step: 70 loss: 0.00035240721460107384\n",
            "epoch: 114 step: 80 loss: 0.00020473121899062827\n",
            "epoch: 114 step: 90 loss: 0.00015493709211776045\n",
            "epoch: 114 step: 100 loss: 0.00022304310852900954\n",
            "epoch: 114 step: 110 loss: 0.00017964055614860466\n",
            "epoch: 114 step: 120 loss: 0.00025025980236049723\n",
            "epoch: 114 step: 130 loss: 0.0001147351090943386\n",
            "epoch: 114 step: 140 loss: 0.00018157793468579056\n",
            "epoch: 114 step: 150 loss: 0.0005318995371507702\n",
            "epoch: 114 step: 160 loss: 0.00019969314441239998\n",
            "epoch: 114 step: 170 loss: 0.00017720324282263894\n",
            "epoch: 114 step: 180 loss: 0.0001719948538071173\n",
            "epoch: 114 finished! loss: 0.0001719948538071173 learning_rate: [2.5e-05]\n",
            "epoch: 115 step: 10 loss: 0.0001551884733085568\n",
            "epoch: 115 step: 20 loss: 0.0004368462246419736\n",
            "epoch: 115 step: 30 loss: 0.00031316755189322545\n",
            "epoch: 115 step: 40 loss: 0.00014067994012179587\n",
            "epoch: 115 step: 50 loss: 0.00020308459455798647\n",
            "epoch: 115 step: 60 loss: 0.00021251817285823063\n",
            "epoch: 115 step: 70 loss: 8.877682681220928e-05\n",
            "epoch: 115 step: 80 loss: 0.00016034968255600746\n",
            "epoch: 115 step: 90 loss: 0.00011302184526642912\n",
            "epoch: 115 step: 100 loss: 8.904467970034647e-05\n",
            "epoch: 115 step: 110 loss: 0.00010227835706582404\n",
            "epoch: 115 step: 120 loss: 0.00033279565254521273\n",
            "epoch: 115 step: 130 loss: 0.00012233112441887382\n",
            "epoch: 115 step: 140 loss: 0.00028584806576928184\n",
            "epoch: 115 step: 150 loss: 0.00026373666830443006\n",
            "epoch: 115 step: 160 loss: 0.0002908871677119235\n",
            "epoch: 115 step: 170 loss: 0.00023125918453584208\n",
            "epoch: 115 step: 180 loss: 0.0002797227292389083\n",
            "epoch: 115 finished! loss: 0.0002797227292389083 learning_rate: [2.5e-05]\n",
            "epoch: 116 step: 10 loss: 8.515706328782237e-05\n",
            "epoch: 116 step: 20 loss: 0.00021358749692328556\n",
            "epoch: 116 step: 30 loss: 8.257502596135159e-05\n",
            "epoch: 116 step: 40 loss: 0.00019772771697691913\n",
            "epoch: 116 step: 50 loss: 0.00010023866729233295\n",
            "epoch: 116 step: 60 loss: 0.0004632366549272139\n",
            "epoch: 116 step: 70 loss: 0.00023404827942807255\n",
            "epoch: 116 step: 80 loss: 8.314645527009392e-05\n",
            "epoch: 116 step: 90 loss: 0.00019717167287418945\n",
            "epoch: 116 step: 100 loss: 9.007287022429368e-05\n",
            "epoch: 116 step: 110 loss: 0.00015395866089336916\n",
            "epoch: 116 step: 120 loss: 0.0002864893885849118\n",
            "epoch: 116 step: 130 loss: 0.0002144526216527022\n",
            "epoch: 116 step: 140 loss: 0.00013964003819449697\n",
            "epoch: 116 step: 150 loss: 7.915145683037887e-05\n",
            "epoch: 116 step: 160 loss: 0.00012436529305056933\n",
            "epoch: 116 step: 170 loss: 9.36036896915979e-05\n",
            "epoch: 116 step: 180 loss: 0.000215678847027576\n",
            "epoch: 116 finished! loss: 0.000215678847027576 learning_rate: [2.5e-05]\n",
            "epoch: 117 step: 10 loss: 0.00018202969314133982\n",
            "epoch: 117 step: 20 loss: 8.524189485900252e-05\n",
            "epoch: 117 step: 30 loss: 0.00011217718379061835\n",
            "epoch: 117 step: 40 loss: 7.26194765200693e-05\n",
            "epoch: 117 step: 50 loss: 0.00031933411497753883\n",
            "epoch: 117 step: 60 loss: 0.00021835224739804915\n",
            "epoch: 117 step: 70 loss: 0.0002014377770541842\n",
            "epoch: 117 step: 80 loss: 0.00025570415846024876\n",
            "epoch: 117 step: 90 loss: 0.0004217097293471542\n",
            "epoch: 117 step: 100 loss: 0.00018238372342693708\n",
            "epoch: 117 step: 110 loss: 0.00016406356673825067\n",
            "epoch: 117 step: 120 loss: 0.0002600537073638894\n",
            "epoch: 117 step: 130 loss: 0.00026537743216187233\n",
            "epoch: 117 step: 140 loss: 8.737195480179661e-05\n",
            "epoch: 117 step: 150 loss: 0.00014257635604110968\n",
            "epoch: 117 step: 160 loss: 0.00039425356694264236\n",
            "epoch: 117 step: 170 loss: 0.00040922994639841325\n",
            "epoch: 117 step: 180 loss: 0.00037845714304181766\n",
            "epoch: 117 finished! loss: 0.00037845714304181766 learning_rate: [2.5e-05]\n",
            "epoch: 118 step: 10 loss: 0.00023441628321235945\n",
            "epoch: 118 step: 20 loss: 0.0002497692701377125\n",
            "epoch: 118 step: 30 loss: 0.000134256403165697\n",
            "epoch: 118 step: 40 loss: 0.00012011790802344876\n",
            "epoch: 118 step: 50 loss: 0.00018979341790522296\n",
            "epoch: 118 step: 60 loss: 0.0004954321738804602\n",
            "epoch: 118 step: 70 loss: 0.0003242778905972866\n",
            "epoch: 118 step: 80 loss: 0.00031106318588791496\n",
            "epoch: 118 step: 90 loss: 0.00012140841621462841\n",
            "epoch: 118 step: 100 loss: 0.00012566983341442733\n",
            "epoch: 118 step: 110 loss: 0.00015574951571868805\n",
            "epoch: 118 step: 120 loss: 0.0002546624892498466\n",
            "epoch: 118 step: 130 loss: 0.0001482648768095258\n",
            "epoch: 118 step: 140 loss: 0.00012092745304659497\n",
            "epoch: 118 step: 150 loss: 0.00014836505141462144\n",
            "epoch: 118 step: 160 loss: 0.0002197246236722221\n",
            "epoch: 118 step: 170 loss: 0.0003076608083226765\n",
            "epoch: 118 step: 180 loss: 0.0002161809090573728\n",
            "epoch: 118 finished! loss: 0.0002161809090573728 learning_rate: [2.5e-05]\n",
            "epoch: 119 step: 10 loss: 0.00017198695936003646\n",
            "epoch: 119 step: 20 loss: 9.28043531328718e-05\n",
            "epoch: 119 step: 30 loss: 0.00029876343044758525\n",
            "epoch: 119 step: 40 loss: 0.00020054340074207117\n",
            "epoch: 119 step: 50 loss: 0.0002746389558587435\n",
            "epoch: 119 step: 60 loss: 8.276636995152868e-05\n",
            "epoch: 119 step: 70 loss: 9.670652495041691e-05\n",
            "epoch: 119 step: 80 loss: 0.00017240686780576003\n",
            "epoch: 119 step: 90 loss: 0.00010282580894363662\n",
            "epoch: 119 step: 100 loss: 0.00021994824773173782\n",
            "epoch: 119 step: 110 loss: 0.00015309900788709966\n",
            "epoch: 119 step: 120 loss: 0.00019787296114934455\n",
            "epoch: 119 step: 130 loss: 0.0003033960552251129\n",
            "epoch: 119 step: 140 loss: 0.00010035461683193428\n",
            "epoch: 119 step: 150 loss: 0.00011661518696575822\n",
            "epoch: 119 step: 160 loss: 9.704487529865685e-05\n",
            "epoch: 119 step: 170 loss: 0.00023674925907709565\n",
            "epoch: 119 step: 180 loss: 0.00017425031336807436\n",
            "epoch: 119 finished! loss: 0.00017425031336807436 learning_rate: [2.5e-05]\n",
            "save last model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT0aKogy9Gnz",
        "colab_type": "code",
        "outputId": "bc10d77b-3abc-4f52-d0f5-20bc27cb8700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!python captcha_test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load cnn net.\n",
            "Test Size: 2000\n",
            "Test Accuracy of the model on the 200 test images: 95.000000 %\n",
            "Test Accuracy of the model on the 400 test images: 95.500000 %\n",
            "Test Accuracy of the model on the 600 test images: 95.333333 %\n",
            "Test Accuracy of the model on the 800 test images: 95.625000 %\n",
            "Test Accuracy of the model on the 1000 test images: 95.700000 %\n",
            "Test Accuracy of the model on the 1200 test images: 96.166667 %\n",
            "Test Accuracy of the model on the 1400 test images: 96.071429 %\n",
            "Test Accuracy of the model on the 1600 test images: 95.750000 %\n",
            "Test Accuracy of the model on the 1800 test images: 95.666667 %\n",
            "Test Accuracy of the model on the 2000 test images: 95.600000 %\n",
            "Test Accuracy of the model on the 2000 test images: 95.600000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb3VvimD9QCr",
        "colab_type": "code",
        "outputId": "e59623c5-6071-4956-ad47-0bdd94e489fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "!python captcha_predict.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load cnn net.\n",
            "Traceback (most recent call last):\n",
            "  File \"captcha_predict.py\", line 71, in <module>\n",
            "    main()\n",
            "  File \"captcha_predict.py\", line 40, in main\n",
            "    fopen = Image.open(file)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2818, in open\n",
            "    prefix = fp.read(16)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtWN0nsdjuAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,Dataset,random_split\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLNMJBgqCzT",
        "colab_type": "text"
      },
      "source": [
        "+ setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYgCt0RiqFBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.chdir(\"drive/My Drive/aimatch\") \n",
        "NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "UPPERCASE = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "LOWERCASE = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "ALL_CHAR_SET = NUMBER + UPPERCASE + LOWERCASE\n",
        "ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)\n",
        "MAX_CAPTCHA = 4\n",
        "\n",
        "# 图像大小\n",
        "# IMAGE_HEIGHT = 60\n",
        "# IMAGE_WIDTH = 160\n",
        "IMAGE_HEIGHT = 40\n",
        "IMAGE_WIDTH = 120\n",
        "\n",
        "# 训练集、测试集、预测集的路径\n",
        "TRAIN_DATASET_PATH = 'dataset' + os.path.sep + 'train'\n",
        "TEST_DATASET_PATH = 'dataset' + os.path.sep + 'test'\n",
        "PREDICT_DATASET_PATH = 'dataset' + os.path.sep + 'predict'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmXcssEejgdA",
        "colab_type": "text"
      },
      "source": [
        "+ onehot编码\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy5_-Tlzj24v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(text):\n",
        "    vector = np.zeros(ALL_CHAR_SET_LEN * MAX_CAPTCHA, dtype=float)\n",
        "    def char2pos(c):\n",
        "        if c =='_':\n",
        "            k = 62\n",
        "            return k\n",
        "        k = ord(c)-48\n",
        "        if k > 9:\n",
        "            k = ord(c) - 65 + 10\n",
        "            if k > 35:\n",
        "                k = ord(c) - 97 + 26 + 10\n",
        "                if k > 61:\n",
        "                    raise ValueError('error')\n",
        "        return k\n",
        "    for i, c in enumerate(text):\n",
        "        idx = i * ALL_CHAR_SET_LEN + char2pos(c)\n",
        "        vector[idx] = 1.0\n",
        "    return vector\n",
        "\n",
        "def decode(vec):\n",
        "    char_pos = vec.nonzero()[0]\n",
        "    text=[]\n",
        "    for i, c in enumerate(char_pos):\n",
        "        char_at_pos = i #c/63\n",
        "        char_idx = c % ALL_CHAR_SET_LEN\n",
        "        if char_idx < 10:\n",
        "            char_code = char_idx + ord('0')\n",
        "        elif char_idx <36:\n",
        "            char_code = char_idx - 10 + ord('A')\n",
        "        elif char_idx < 62:\n",
        "            char_code = char_idx - 36 + ord('a')\n",
        "        elif char_idx == 62:\n",
        "            char_code = ord('_')\n",
        "        else:\n",
        "            raise ValueError('error')\n",
        "        text.append(chr(char_code))\n",
        "    return \"\".join(text)\n",
        "\n",
        "# e = encode(\"BK7H\")\n",
        "# print(e[62:124])\n",
        "# print(decode(e))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBpxHpC9kM1h",
        "colab_type": "text"
      },
      "source": [
        "+ 数据集加载"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3i2OKXhkd5n",
        "colab_type": "code",
        "outputId": "c9734e35-a621-414a-ae97-25bc8aad7c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "split_rate = 0.1 # 划分训练集和测试集的比率\n",
        "torch.manual_seed(1) # 设置随机数种子为固定值\n",
        "\n",
        "class mydataset(Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.train_image_file_paths = [os.path.join(folder, image_file).replace('\\\\','/') for image_file in os.listdir(folder)]\n",
        "        # print('train_image_file_paths:',self.train_image_file_paths)\n",
        "        csv_path= 'dataset/train_label.csv'# os.path.join(folder,'train_label.csv').replace('\\\\','/') #dataset/train/train_label.csv\n",
        "        data=pd.read_csv(csv_path)\n",
        "        self.labels=np.array(data['label'])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_image_file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_root = self.train_image_file_paths[idx]\n",
        "        image = Image.open(image_root)\n",
        "        image = image.resize((120, 40), Image.BICUBIC)      # 105*35 -> 120*40\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        '''版本1：当图片文件名为标签名称时，通过'.'分割取出每张图片的标签存入label\n",
        "        image_name = image_root.split('/')[-1]\n",
        "        label = ohe.encode(image_name.split('.')[0]) # 为了方便，在生成图片的时候，图片文件的命名格式 \"4个数字或者数字_时间戳.PNG\", 4个字母或者即是图片的验证码的值，字母大写,同时对该值做 one-hot 处理\n",
        "        '''\n",
        "        # 版本2 保留原有的数据格式，通过csv文件获取每张图片标签而不用对每张图片进行重命名打上标签\n",
        "        image_name = image_root.split('/')[-1] #xxxx.jpg\n",
        "        image_num = image_name.split('.')[0] # 第xxxx张图片\n",
        "        #print('图片名称',image_name,self.labels[int(image_num)-1])\n",
        "        label=encode(self.labels[int(image_num)-1])\n",
        "        return image, label\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    # transforms.ColorJitter(),\n",
        "    # transforms.Grayscale(),           # 可以尝试转灰度图处理，输入通道就要改成1\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 创建数据集对象\n",
        "dataset = mydataset(TRAIN_DATASET_PATH, transform=transform)\n",
        "# print(len(dataset))\n",
        "# 测试集的长度\n",
        "test_size = int(split_rate*len(dataset))\n",
        "# print(test_size)\n",
        "# 训练集的长度\n",
        "train_size = int(len(dataset)-test_size)\n",
        "# print(train_size)\n",
        "# 随机划分\n",
        "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "def get_train_data_loader(batch_size=64):\n",
        "    # dataset = mydataset(captcha_setting.TRAIN_DATASET_PATH, transform=transform)\n",
        "    # return DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "    return DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=16)\n",
        "\n",
        "def get_test_data_loader():\n",
        "    # dataset = mydataset(captcha_setting.TEST_DATASET_PATH, transform=transform)\n",
        "    # return DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    return DataLoader(test_ds, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "# def get_predict_data_loader():\n",
        "#     dataset = mydataset(captcha_setting.PREDICT_DATASET_PATH, transform=transform)\n",
        "#     return DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "'''这个方法不用了，Predict模块自写了顺序遍历'''\n",
        "def get_predict_data_loader():\n",
        "    dataset = mydataset(PREDICT_DATASET_PATH, transform=transform)\n",
        "    return DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "test_loader = get_test_data_loader()\n",
        "for i, (im, lb) in enumerate(test_loader):\n",
        "    print(i, im[i].shape, lb[i].shape)\n",
        "    if i == 0: break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([3, 40, 120]) torch.Size([248])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mP7lSyxRBQx",
        "colab_type": "text"
      },
      "source": [
        "+ 模型准确率"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7WDjyNykGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_data(model):\n",
        "    model.eval()\n",
        "    test_dataloader = get_test_data_loader()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_dataloader):\n",
        "        image = images\n",
        "        vimage = Variable(image).cuda()\n",
        "        predict_label = model(vimage).cuda()\n",
        "\n",
        "        c0 = ALL_CHAR_SET[np.argmax(\n",
        "            predict_label[0, 0:ALL_CHAR_SET_LEN].data.cpu().numpy())]\n",
        "        c1 = ALL_CHAR_SET[np.argmax(\n",
        "            predict_label[0, ALL_CHAR_SET_LEN:2 * ALL_CHAR_SET_LEN].data.cpu().numpy())]\n",
        "        c2 = ALL_CHAR_SET[np.argmax(\n",
        "            predict_label[0, 2 * ALL_CHAR_SET_LEN:3 * ALL_CHAR_SET_LEN].data.cpu().numpy())]\n",
        "        c3 = ALL_CHAR_SET[np.argmax(\n",
        "            predict_label[0, 3 * ALL_CHAR_SET_LEN:4 * ALL_CHAR_SET_LEN].data.cpu().numpy())]\n",
        "        predict_label = '%s%s%s%s' % (c0, c1, c2, c3)\n",
        "        true_label = decode(labels.numpy()[0])\n",
        "        total += labels.size(0)\n",
        "        if(predict_label == true_label):\n",
        "            correct += 1\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTA196VKq4ni",
        "colab_type": "text"
      },
      "source": [
        "+ VGG16模型\n",
        "\n",
        "  单通道"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe3cr4Eqq1lS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN2, self).__init__()\n",
        "        '''输入改为3，因为RGB3通道，原来是1'''\n",
        "        self.layer1 = self.make_layers([64, 64, 'M'], 1, droup_out=False) \n",
        "        self.layer2 = self.make_layers([128, 128, 'M'], 64, droup_out=True)\n",
        "        self.layer3 = self.make_layers(\n",
        "            [256, 256, 256, 'M', 512, 512, 512, 'M'], 128, droup_out=False)\n",
        "        self.layer4 = self.make_layers(\n",
        "            [512, 512, 512, 'M'], 512, droup_out=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear((IMAGE_WIDTH//32)*(IMAGE_HEIGHT//32)*512, 1024),\n",
        "            nn.Dropout(0.5),  # drop 50% of the neuron\n",
        "            nn.LeakyReLU())\n",
        "        self.rfc = nn.Sequential(nn.Linear(1024, MAX_CAPTCHA*ALL_CHAR_SET_LEN))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = out.view(out.size(0), -1) #-1, out.size(0)\n",
        "        # print(\"-->{}\".format(out.size()))\n",
        "        out = self.fc(out)\n",
        "        out = self.rfc(out)\n",
        "        return out\n",
        "\n",
        "    def make_layers(self, cfg, inc, droup_out=False):\n",
        "        layers = []\n",
        "        i = inc\n",
        "        for v in cfg:\n",
        "            if v == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(i, v, kernel_size=3, padding=1)\n",
        "                if droup_out:\n",
        "                    layers += [conv2d,\n",
        "                               nn.BatchNorm2d(v), nn.Dropout(0.5), nn.LeakyReLU()]\n",
        "                else:\n",
        "                    layers += [conv2d, nn.BatchNorm2d(v), nn.LeakyReLU()]\n",
        "                i = v\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def model_name(self):\n",
        "        return self.__class__.__name__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLKH51lEjDgp",
        "colab_type": "text"
      },
      "source": [
        "+ ResNet18模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRaK1n0hjHSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, inchannel, outchannel, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.left = nn.Sequential(\n",
        "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(outchannel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(outchannel)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inchannel != outchannel:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(outchannel)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.left(x)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResidualBlock, num_classes=64):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inchannel = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
        "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
        "        # self.fc = nn.Linear(512, MAX_CAPTCHA*ALL_CHAR_SET_LEN)\n",
        "        # self.fc = nn.Sequential(\n",
        "        #     nn.Linear((IMAGE_WIDTH // 8) * (IMAGE_HEIGHT // 8) * 512, 512),\n",
        "        #     nn.Dropout(0.5),  # drop 50% of the neuron\n",
        "        #     nn.LeakyReLU())\n",
        "        self.rfc = nn.Sequential(nn.Linear(512, MAX_CAPTCHA * ALL_CHAR_SET_LEN))\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "\n",
        "    def make_layer(self, block, channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.inchannel, channels, stride))\n",
        "            self.inchannel = channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)                  # (batch_size, 512, 5, 15)\n",
        "        out = nn.AdaptiveAvgPool2d(1)(out)      # (batch_size, 512, 1, 1)\n",
        "        out = out.view(-1, 512)                 # (batch_size, 512)\n",
        "        out = self.drop(out)\n",
        "        out = self.rfc(out)                     # 248\n",
        "        return out\n",
        "    \n",
        "    def model_name(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(ResidualBlock)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BNCH8LkvDsc",
        "colab_type": "text"
      },
      "source": [
        "+ 训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gum2aHHovGEg",
        "colab_type": "code",
        "outputId": "15128f48-9a51-4762-e1a9-77cc8a0077df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 如果路径不存在，创建\n",
        "if not os.path.isdir(\"./checkpoint\"):\n",
        "    os.mkdir(\"./checkpoint\")\n",
        "\n",
        "# 断点续训标识\n",
        "RESUME = False\n",
        "checkpoint_path = r'./checkpoint/ckpt_ResNet18_second_times-model.pth' #r'./checkpoint/ckpt_model.pth'\n",
        "\n",
        "# Hyper Parameters\n",
        "num_epochs = 100\n",
        "# num_epochs = 1\n",
        "# batch_size = 100\n",
        "batch_size = 100\n",
        "learning_rate = 3e-4\n",
        "acc_max=0\n",
        "\n",
        "# Train the Model\n",
        "train_dataloader = get_train_data_loader(batch_size)\n",
        "\n",
        "model = ResNet18() # CNN2()\n",
        "model.train()\n",
        "\n",
        "# 模型移入GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "print('Init ## {} ## model.'.format(model.model_name()))\n",
        "# print(model)\n",
        "\n",
        "# 开始训练的epoch\n",
        "start_epoch = -1\n",
        "\n",
        "# 如果RESUME==True，加载已保存的模型\n",
        "if RESUME:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['start_epoch']\n",
        "    acc_max=checkpoint['acc']\n",
        "    #loss=checkpoint['loss']\n",
        "    print('Continue training from epoch {}...'.format(start_epoch))\n",
        "\n",
        "criterion = nn.MultiLabelSoftMarginLoss().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    \n",
        "\n",
        "#print('start_epoch+1:',start_epoch+1,'载入模型的acc:',checkpoint['acc'],\"载入模型的loss\",checkpoint['loss'])\n",
        "for epoch in range(start_epoch+1 ,num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_dataloader):\n",
        "        # 数据移入GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        predict_labels = model(images)\n",
        "        loss = criterion(predict_labels, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(\"epoch:\", epoch, \"step:\", i+1, \"loss:\", loss.item())\n",
        "\n",
        "        # if (i+1) % 100 == 0:\n",
        "        #     torch.save(model.state_dict(), \"model.pkl\")   #current is model.pkl\n",
        "        #     print(\"save model\")\n",
        "        numCor = test_data(model) #函数返回的是准确率\n",
        "        pthname = \"{0}_epoch{1}_loss{2}_acc_{3}.pth\".format(\n",
        "            model.model_name(), epoch, str(loss.item())[:8], numCor)\n",
        "\n",
        "    if numCor>acc_max:\n",
        "      checkpoint = {\n",
        "        'model':model.state_dict(),\n",
        "        'optimizer':optimizer.state_dict(),\n",
        "        'start_epoch': epoch,\n",
        "        'loss': loss.item(),\n",
        "        'acc': numCor\n",
        "      }\n",
        "      acc_max=numCor\n",
        "      torch.save(checkpoint, './checkpoint/ckpt_ResNet18_6.2-model.pth')\n",
        "      print(\"save model:\"+pthname)\n",
        "      print('model loss:',checkpoint['loss'],'model acc:',checkpoint['acc'])\n",
        "    else:\n",
        "      print('no save, acc_max:',acc_max, '当前acc:',numCor,'loss:',loss.item())\n",
        "    print(\"epoch:\", epoch, \"finished!\", \"loss:\", loss.item())\n",
        "\n",
        "    #if (epoch) % 5 == 0 and epoch != 0:\n",
        "\n",
        "    # numCor = test_data(model) #函数返回的是准确率\n",
        "    # pthname = \"{0}_epoch{1}_loss{2}_acc_{3}.pth\".format(\n",
        "    #     model.model_name(), epoch, str(loss.item())[:8], numCor)\n",
        "\n",
        "    # if numCor>acc_max:\n",
        "      \n",
        "    #   checkpoint = {\n",
        "    #     'model':model.state_dict(),\n",
        "    #     'optimizer':optimizer.state_dict(),\n",
        "    #     'start_epoch': epoch,\n",
        "    #     'loss': loss.item(),\n",
        "    #     'acc': numCor\n",
        "    #   }\n",
        "\n",
        "    #   acc_max=numCor\n",
        "    #   torch.save(checkpoint, './checkpoint/ckpt_ResNet18_second_times-model.pth')\n",
        "    #   print(\"save model:\"+pthname)\n",
        "    #   print('model loss:',checkpoint['loss'],'model acc:',checkpoint['acc'])\n",
        "    # else:\n",
        "    #   print('no save, acc_max:',acc_max, '当前acc:',numCor,'loss:',loss.item())\n",
        "    \n",
        "#torch.save(model.state_dict(), \"./model/{0}.pkl\".format(model.model_name()))\n",
        "# torch.save(model.state_dict(), \"model.pkl\")               #current is model.pkl\n",
        "torch.save(checkpoint, './checkpoint/ckpt_ResNet18_last.pth')\n",
        "print(\"save last model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init ## ResNet ## model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnqw1mWGxdb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi\n",
        "#os.mkdir(\"./checkpoint\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXDkQlmhRxA",
        "colab_type": "text"
      },
      "source": [
        "+ 测试模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reAReTxBhWtU",
        "colab_type": "code",
        "outputId": "bc8f2228-6e14-48e7-86fc-1b33c77902ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "model = ResNet18()\n",
        "model.eval()\n",
        "model.load_state_dict(torch.load('checkpoint/ckpt_ResNet18_second_times-model.pth')['model'])\n",
        "print(\"load model net.\")\n",
        "\n",
        "test_dataloader = get_test_data_loader()\n",
        "print('Test Size:', len(test_dataloader.dataset))\n",
        "correct = 0\n",
        "total = 0\n",
        "for i, (images, labels) in enumerate(test_dataloader):\n",
        "    # vimage = Variable(image)\n",
        "    predict_label = model(images)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(predict_label[0, 0:ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(predict_label[0, ALL_CHAR_SET_LEN:2 * ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(predict_label[0, 2 * ALL_CHAR_SET_LEN:3 * ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(predict_label[0, 3 * ALL_CHAR_SET_LEN:4 * ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    predict_label = '%s%s%s%s' % (c0, c1, c2, c3)\n",
        "    true_label = decode(labels.numpy()[0])\n",
        "    # print(predict_label, true_label)\n",
        "    total += labels.size(0)\n",
        "    if predict_label == true_label:\n",
        "        correct += 1\n",
        "    if total % 200 == 0:\n",
        "        print('Test Accuracy of the model on the %d test images: %f %%' % (total, 100 * correct / total))\n",
        "print('Test Accuracy of the model on the %d test images: %f %%' % (total, 100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load model net.\n",
            "Test Size: 2000\n",
            "Test Accuracy of the model on the 200 test images: 98.500000 %\n",
            "Test Accuracy of the model on the 400 test images: 96.750000 %\n",
            "Test Accuracy of the model on the 600 test images: 95.500000 %\n",
            "Test Accuracy of the model on the 800 test images: 95.125000 %\n",
            "Test Accuracy of the model on the 1000 test images: 94.400000 %\n",
            "Test Accuracy of the model on the 1200 test images: 94.416667 %\n",
            "Test Accuracy of the model on the 1400 test images: 94.214286 %\n",
            "Test Accuracy of the model on the 1600 test images: 94.250000 %\n",
            "Test Accuracy of the model on the 1800 test images: 94.500000 %\n",
            "Test Accuracy of the model on the 2000 test images: 94.600000 %\n",
            "Test Accuracy of the model on the 2000 test images: 94.600000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c21dTT3pZ-l8",
        "colab_type": "text"
      },
      "source": [
        "+ 预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lptoNz3paAiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "csv_file = 'submission.csv'\n",
        "# predict文件的路径\n",
        "# os.mkdir\n",
        "pathDir = os.listdir('dataset/test/')\n",
        "\n",
        "f = open(csv_file, \"w\", newline='')\n",
        "csv_writer = csv.writer(f)\n",
        "csv_writer.writerow([\"ID\",\"label\"])\n",
        "\n",
        "# cnn = CNN()           # 采用模型1\n",
        "model = ResNet18()            # 采用模型2\n",
        "model.eval()\n",
        "model.load_state_dict(torch.load('checkpoint/ckpt_ResNet18_model.pth')['model'])\n",
        "print(\"load model net.\")\n",
        "\n",
        "# 按照文件名（除去格式部分）排序\n",
        "pathDir.sort(key=lambda x: int(x[:-4]))\n",
        "print('pathDir:',pathDir)\n",
        "for allDir in pathDir:\n",
        "    file = os.path.join('%s%s' % ('dataset/test/', allDir))\n",
        "    print('file:',file)\n",
        "    fopen = Image.open(file)\n",
        "\n",
        "    image = fopen.resize((120, 40), Image.BICUBIC)\n",
        "    image = transform(image)\n",
        "    image = torch.unsqueeze(image, dim=0)\n",
        "\n",
        "    predict_label = model(image)\n",
        "\n",
        "    c0 = ALL_CHAR_SET[np.argmax(predict_label[0, 0:ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(\n",
        "        predict_label[0, ALL_CHAR_SET_LEN:2 * ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(\n",
        "        predict_label[0, 2 * ALL_CHAR_SET_LEN:3 * ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(\n",
        "        predict_label[0, 3 * ALL_CHAR_SET_LEN:4 * ALL_CHAR_SET_LEN].data.numpy())]\n",
        "    label = '%s%s%s%s' % (c0, c1, c2, c3)\n",
        "\n",
        "    ID = str(allDir)\n",
        "    csv_writer.writerow([ID, label])\n",
        "    print(ID, label)\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}